{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6062c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import models as M\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "class BahdanauAttention(L.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = L.Dense(units)\n",
    "        self.W2 = L.Dense(units)\n",
    "        self.V = L.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query - shape == (batch_size, hidden_size) -> decoder hidden state at the current timestep\n",
    "        # values - shape == (batch_size, max_len/timesteps, hidden_size) -> encoder outputs (all timesteps)\n",
    "        # here, hidden_size = units, max_len = timesteps\n",
    "        query = tf.expand_dims(query, axis = 1)                # (batch_size, 1, hidden_size)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query) + self.W2(values)))  # (batch_size, timesteps, 1)\n",
    "        attention_weight = tf.nn.softmax(score, axis = 1)      # (batch_size, timesteps, 1)\n",
    "        context = attention_weight*values                      # (batch_size, timesteps, hidden_size)\n",
    "        context_vector = tf.reduce_sum(context, axis = 1)      # (batch_size, hidden_size)\n",
    "        return context_vector, attention_weight\n",
    "class Encoder(L.Layer):\n",
    "    def __init__(self, in_vocab, embedding_dim, hidden_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = L.Embedding(in_vocab, embedding_dim)       # (batch_size, seq_length) -> (batch_size, seq_length, embedding_dim)\n",
    "        self.lstm = L.LSTM(hidden_units, return_sequences=True,return_state = True)   # (batch_size, seq_length, embedding_dim) -> (batch_size, hidden_units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # input : (batch_size, seq_length)\n",
    "        x = self.embed(inputs)                               # (batch_size, seq_length, embeddign_dim)\n",
    "        enc_out, hidden_state, cell_state = self.lstm(x)     # O/P (batch_size, seq_len, hidden_size)\n",
    "        return enc_out, hidden_state, cell_state\n",
    "class Decoder(L.Layer):\n",
    "    def __init__(self, out_vocab, embedding_dim, hidden_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = L.Embedding(out_vocab, embedding_dim)     # (batch_size, seq_length) -> (batch_size, seq_length, embedding_dim)\n",
    "        self.lstm = L.LSTM(hidden_units, return_sequences = True, return_state = True)  # (batch_size, seq_length, embedding_dim) -> (batch_size, hidden_units)\n",
    "        self.dense = L.Dense(out_vocab, activation='softmax')  # (batch_size, seq_length, hidden_units) -> (batch_size, seq_length, out_vocab)\n",
    "        self.attention = BahdanauAttention(64)\n",
    "    \n",
    "    def call(self, inputs, hidden_state, cell_state, enc_output):\n",
    "        # input : (batch_size, 1)\n",
    "        x = self.embed(inputs)                                 # (batch_size, 1, embedding_dim)\n",
    "        states = [hidden_state, cell_state] \n",
    "        context, attention_weights = self.attention(query = hidden_state, values = enc_output)\n",
    "        dec_out, hidden_state, cell_state = self.lstm(x, initial_state=states)  # O/P : (batch_size, 1, hidden_units)\n",
    "        dec_out = tf.squeeze(dec_out, axis=1)                  # (batch_size, hidden_units)\n",
    "        # context = tf.expand_dims(context, axis=1)              # (batch_size, 1, embedding_dim)\n",
    "        inputs = tf.concat([context, dec_out], axis=-1)        # (batch_size, 1, embedding_dim + enc_units)\n",
    "        out = self.dense(inputs)                               # (batch_size, 1, out_vocab)\n",
    "        return out, hidden_state, cell_state \n",
    "\n",
    "@keras.saving.register_keras_serializable(package=\"Custom\", name=\"Seq2Seq\")\n",
    "class Seq2Seq(M.Model):\n",
    "\n",
    "    def __init__(self, in_vocab, out_vocab, embedding_dim, hidden_units, end_token):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.in_vocab = in_vocab\n",
    "        self.out_vocab = out_vocab\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        self.encoder = Encoder(in_vocab, embedding_dim, hidden_units)\n",
    "        self.decoder = Decoder(out_vocab, embedding_dim, hidden_units)\n",
    "        self.end_token = end_token\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, inputs):\n",
    "        (enc_inputs, dec_inputs), targets = inputs         # (batch_size, seq_length)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_out, hidden_state, cell_state = self.encoder(enc_inputs)           # (batch_size, hidden_units)\n",
    "            seq_len = dec_inputs.shape[1]\n",
    "            dec_out = tf.TensorArray(tf.float32, seq_len)  # (batch_size, seq_len, target_vocab_size)\n",
    "            mask = tf.TensorArray(tf.bool, size=seq_len)\n",
    "            for timestep in tf.range(seq_len):\n",
    "                timestep_input = dec_inputs[:, timestep:timestep+1]       # (batch_size, 1)\n",
    "                timestep_output, hidden_state, cell_state = self.decoder(timestep_input, hidden_state, cell_state, enc_out)   # timestep_output -> # (batch_size, 1, hidden_units)\n",
    "                dec_out = dec_out.write(timestep, timestep_output)\n",
    "                is_end = tf.equal(targets[:, timestep], self.end_token)  # Creating mask based on whether end token is encountered\n",
    "                mask = mask.write(timestep, tf.logical_not(is_end))\n",
    "            dec_out = tf.transpose(dec_out.stack(), [1, 0, 2])\n",
    "            sequence_mask = tf.transpose(mask.stack(), [1, 0])\n",
    "            loss = self.compiled_loss(targets, dec_out, sample_weight=tf.cast(sequence_mask, tf.float32))   \n",
    "        variables = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        self.compiled_metrics.update_state(targets, dec_out) # Update metrics\n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_inputs, dec_inputs = inputs                       \n",
    "        enc_out, hidden_state, cell_state = self.encoder(enc_inputs)   # (batch_size, hidden_units)\n",
    "        seq_len = tf.shape(dec_inputs)[1]\n",
    "        dec_out = tf.TensorArray(tf.float32, seq_len)  # (batch_size, seq_len, target_vocab_size)\n",
    "        for timestep in tf.range(seq_len):\n",
    "            timestep_input = dec_inputs[:, timestep:timestep+1]       # (batch_size, 1)\n",
    "            timestep_output, hidden_state, cell_state = self.decoder(timestep_input, hidden_state, cell_state, enc_out)   # timestep_output -> # (batch_size, 1, hidden_units)\n",
    "            dec_out = dec_out.write(timestep, timestep_output)\n",
    "        return tf.transpose(dec_out.stack(), [1, 0, 2])\n",
    "    \n",
    "\n",
    "    def generate(self, enc_inputs, max_len, start, end):\n",
    "        enc_out, hidden_state, cell_state = self.encoder(enc_inputs)\n",
    "        dec_in = tf.expand_dims([start], 0)              # To get from int -> (1,1) tensor\n",
    "        result = []\n",
    "        for _ in range(max_len): \n",
    "            prediction_logits, hidden_state, cell_state = self.decoder(dec_in, hidden_state, cell_state, enc_out) # (1, 1, hidden_units)\n",
    "            prediction = tf.argmax(prediction_logits, axis=-1)        # return token ID (int)\n",
    "            if prediction == end:\n",
    "                break\n",
    "            result.append(prediction.numpy())\n",
    "            dec_in = tf.expand_dims(prediction, 0) \n",
    "        return result\n",
    "    def get_config(self):\n",
    "        config = super(Seq2Seq, self).get_config()\n",
    "        config.update({\n",
    "            'in_vocab': self.in_vocab,\n",
    "            'out_vocab': self.out_vocab,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'hidden_units': self.hidden_units,\n",
    "            'end_token': self.end_token  # üõ†Ô∏è include this!\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        end_token = config.get('end_token', 0)  # üõ†Ô∏è set a default or handle gracefully\n",
    "        return cls(\n",
    "            in_vocab=config['in_vocab'],\n",
    "            out_vocab=config['out_vocab'],\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            hidden_units=config['hidden_units'],\n",
    "            end_token=end_token\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "234b753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article:\n",
      " (CNN) -- A typhoon slinging fierce winds moved north Saturday toward the Japanese island of Okinawa, on a track to hit the Korean Peninsula, where dozens of people were killed by a big storm last month. Tropical cyclone Sanba had winds of 232 kilometers per hour (144 mph), said CNNI Weather Anchor Jenny Harrison. \"One expects and assumes that people are beginning to already take serious precautions as to the arrival of this very strong typhoon,\" she said. She predicted that storm surge could prove to be a problem for islanders. \"It's a large storm and it's going to have a fairly wide-reaching effect,\" she said. \"Okinawa is pretty much in the path of this storm.\" The storm had been, \"for a very short time,\" classified as a \"super typhoon,\" with winds of more than 241 mph (150 mph), she said. Typhoon tourism: One week in North Korea . Sanba is expected to approach Okinawa late Saturday or early Sunday local time before trudging on toward South Korea, according to projections from regional weather agencies. It is forecast to gradually weaken as it moves north. The Japanese Meteorological Agency on Friday classified Sanba's scale as \"large\" and intensity as \"violent.\" Last month, Typhoon Bolaven killed more than 60 people on the Korean Peninsula. Bolaven had also swept over Okinawa, which escaped relatively unscathed. North Korea rebuffs typhoon aid offer from South . The infrastructure on Okinawa is designed to withstand powerful storms, since the island is in an area of the western Pacific Ocean where typhoons are frequent. \n",
      "\n",
      "Generated Summary:\n",
      " typhoon sanba has sustained winds of 144 mph near its center it is expected to approach okinawa late saturday or early sunday before heading to south korea last month another large storm killed more than 60 people on the korea peninsula storm korea has been trying to approach for the region tonight held by august but is expected to approach higher than 100 people to reach the south korea register its island this weekend japan is expected to approach higher than last month in south korea or 144 on island korea has approach to japan\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Resolve absolute path of the .keras model archive\n",
    "BASE_DIR = os.getcwd()\n",
    "keras_archive = os.path.join(BASE_DIR, \"Attention_Model_(teacher_forcing).keras\")\n",
    "\n",
    "# Manually extract the .keras archive to a temporary directory to avoid internal cleanup issues\n",
    "tmp_dir = tempfile.mkdtemp()\n",
    "with zipfile.ZipFile(keras_archive, 'r') as zip_ref:\n",
    "    zip_ref.extractall(tmp_dir)\n",
    "\n",
    "try:\n",
    "    # Load the extracted SavedModel directory\n",
    "    model = load_model(tmp_dir, custom_objects={\"Seq2Seq\": Seq2Seq})\n",
    "finally:\n",
    "    # Ensure the temporary directory is removed after loading\n",
    "    shutil.rmtree(tmp_dir)\n",
    "\n",
    "# Load tokenizers\n",
    "e_tk_path = os.path.join(BASE_DIR, \"e_tk.pkl\")\n",
    "d_tk_path = os.path.join(BASE_DIR, \"d_tk.pkl\")\n",
    "with open(e_tk_path, \"rb\") as f:\n",
    "    e_tk = pickle.load(f)\n",
    "with open(d_tk_path, \"rb\") as f:\n",
    "    d_tk = pickle.load(f)\n",
    "\n",
    "# Load metadata\n",
    "metadata_path = os.path.join(BASE_DIR, \"metadata.pkl\")\n",
    "with open(metadata_path, \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "word_dict = metadata[\"word_dict\"]\n",
    "start_id = metadata[\"start_id\"]\n",
    "end_id = metadata[\"end_id\"]\n",
    "input_seq_len = metadata[\"input_seq_len\"]\n",
    "output_seq_len = metadata[\"output_seq_len\"]\n",
    "\n",
    "def generate_summary(text):\n",
    "    # Preprocess input text to sequence\n",
    "    seq = e_tk.texts_to_sequences([text])\n",
    "    seq = pad_sequences(seq, maxlen=input_seq_len, padding='post')\n",
    "\n",
    "    # Generate summary tokens\n",
    "    model_output = model.generate(seq, output_seq_len, start_id, end_id)\n",
    "\n",
    "    # Convert token IDs back to words\n",
    "    output_words = []\n",
    "    for token_id in model_output:\n",
    "        # Handle potential array-like tokens\n",
    "        token_id = int(token_id[0]) if hasattr(token_id, '__len__') else int(token_id)\n",
    "        if token_id == end_id:\n",
    "            break\n",
    "        word = word_dict.get(token_id, '')\n",
    "        if word:\n",
    "            output_words.append(word)\n",
    "    return ' '.join(output_words)\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\mitta\\OneDrive - iiit-b\\Documents\\NLP_Project\\Backend\\summariser-api\\filtered_train.csv\")\n",
    "\n",
    "# Pick a random article\n",
    "random_row = df.sample(1).iloc[0]\n",
    "article = random_row['article']  # Replace with the correct column name if it's different\n",
    "\n",
    "# Display the original article (optional)\n",
    "print(\"Original Article:\\n\", article, \"\\n\")\n",
    "\n",
    "# Generate summary\n",
    "summary = generate_summary(article)\n",
    "\n",
    "# Display the summary\n",
    "print(\"Generated Summary:\\n\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
