{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='text-align: center;text-color:blue;'><strong>Sequnce to Sequence modelling</strong></h1>\n",
    "<h2 style='text-align: center;text-color:blue;'>With Teacher Forcing and Attention Mechanism</h2>\n",
    "\n",
    "This notebook explores the implementation of a Sequence-to-Sequence (seq2seq) model with attention and teacher forcing for the task of text summarization.\n",
    "\n",
    "Background:\n",
    "+ **Text Summarization**: The process of condensing a longer piece of text (e.g., an article, document) into a shorter version while preserving the most important information. \n",
    "+ **Seq2seq Models**: A class of neural networks designed to handle sequence-to-sequence tasks, such as machine translation, text summarization, and question answering. They consist of an encoder that processes the input sequence and a decoder that generates the output sequence.  \n",
    "+ **Attention Mechanism**: A key component in modern seq2seq models that allows the decoder to focus on different parts of the input sequence when generating each output token. This improves the model's ability to capture long-range dependencies and produce more accurate translations or summaries. \n",
    "+ **Teacher Forcing**: A training technique where the ground truth output tokens are fed as input to the decoder during training. This helps stabilize training and improve the quality of the generated output, especially in the early stages of training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='text-align:center;'>Seq2Seq models</h2>\n",
    "Seq2Seq models are a type of neural network architecture designed to handle tasks involving sequential data, such as machine translation and text summarization. They consist of two main components: an encoder, which processes the input sequence and creates a context vector, and a decoder, which generates the output sequence based on the context vector. Seq2Seq models have revolutionized many NLP tasks by effectively transforming one sequence into another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='text-align:center;'>Importing Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-11T06:31:12.872874Z",
     "iopub.status.busy": "2025-04-11T06:31:12.872605Z",
     "iopub.status.idle": "2025-04-11T06:31:27.657797Z",
     "shell.execute_reply": "2025-04-11T06:31:27.656923Z",
     "shell.execute_reply.started": "2025-04-11T06:31:12.872846Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 06:31:14.544031: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744353074.745003      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744353074.804322      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import models as M\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><strong><span style=\"font-size: 24px;\">Filtering Training Data Based on Length Constraints</span></strong></p>\n",
    "\n",
    "To prevent **GPU resource exhaustion errors on Kaggle**, the training data is filtered before model training:\n",
    "\n",
    "- Articles and summaries that exceed predefined limits are removed:\n",
    "  - `TEXT_SIZE` for the input article length\n",
    "  - `SUMM_SIZE` for the target summary length\n",
    "- This ensures efficient usage of limited GPU memory during training.\n",
    "\n",
    "Filtering helps keep the dataset manageable while still retaining high-quality training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:31:34.600987Z",
     "iopub.status.busy": "2025-04-11T06:31:34.600358Z",
     "iopub.status.idle": "2025-04-11T06:31:34.604728Z",
     "shell.execute_reply": "2025-04-11T06:31:34.603847Z",
     "shell.execute_reply.started": "2025-04-11T06:31:34.600963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TEXT_SIZE = 1600\n",
    "SUMM_SIZE = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='text-align:center'>About the data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center'>Link : <a>https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail</a> <br><br>The CNN / DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T18:39:36.261389Z",
     "iopub.status.busy": "2025-04-10T18:39:36.260682Z",
     "iopub.status.idle": "2025-04-10T18:40:04.265641Z",
     "shell.execute_reply": "2025-04-10T18:40:04.264943Z",
     "shell.execute_reply.started": "2025-04-10T18:39:36.261356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T18:40:08.883235Z",
     "iopub.status.busy": "2025-04-10T18:40:08.882914Z",
     "iopub.status.idle": "2025-04-10T18:40:09.241595Z",
     "shell.execute_reply": "2025-04-10T18:40:09.240897Z",
     "shell.execute_reply.started": "2025-04-10T18:40:08.883216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18775"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[train['article'].apply(lambda x: len(x)<TEXT_SIZE)]\n",
    "train = train[train['highlights'].apply(lambda x: len(x)<SUMM_SIZE)]\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T18:40:14.978548Z",
     "iopub.status.busy": "2025-04-10T18:40:14.977789Z",
     "iopub.status.idle": "2025-04-10T18:40:14.996710Z",
     "shell.execute_reply": "2025-04-10T18:40:14.995614Z",
     "shell.execute_reply.started": "2025-04-10T18:40:14.978522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = train.reset_index().drop(['index','id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:43:08.379466Z",
     "iopub.status.busy": "2025-04-06T07:43:08.378801Z",
     "iopub.status.idle": "2025-04-06T07:43:08.391222Z",
     "shell.execute_reply": "2025-04-06T07:43:08.390354Z",
     "shell.execute_reply.started": "2025-04-06T07:43:08.379432Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kabul, Afghanistan (CNN) -- China's top securi...</td>\n",
       "      <td>China's top security official visited Afghanis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(CNN) -- Virgin, a leading branded venture cap...</td>\n",
       "      <td>The Virgin Group was founded by Richard Branso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>By . Chris Pleasance . Police are hunting for ...</td>\n",
       "      <td>Two men filmed taking iPad from canoe rental o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baghdad (CNN) -- Radical Iraqi cleric Muqtada ...</td>\n",
       "      <td>Muqtada al-Sadr has been in Iran since 2007 .\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PUBLISHED: . 07:04 EST, 9 January 2014 . | . U...</td>\n",
       "      <td>Zhu Sanni, 23, had been left alone at home for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kabul, Afghanistan (CNN) -- Thousands of bottl...</td>\n",
       "      <td>Official: Bottles are almost exclusively from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(CNN) -- Tour de France race director Christia...</td>\n",
       "      <td>The 2013 Tour de France will start from the Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(CNN) -- Hundreds filed by a casket on Sunday ...</td>\n",
       "      <td>Wes Leonard collapsed after scoring a winning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Earlier this season I picked Thierry Henry as ...</td>\n",
       "      <td>Sportsmail columnist Martin Keown was honoured...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "1  Kabul, Afghanistan (CNN) -- China's top securi...   \n",
       "2  (CNN) -- Virgin, a leading branded venture cap...   \n",
       "3  By . Chris Pleasance . Police are hunting for ...   \n",
       "4  Baghdad (CNN) -- Radical Iraqi cleric Muqtada ...   \n",
       "5  PUBLISHED: . 07:04 EST, 9 January 2014 . | . U...   \n",
       "6  Kabul, Afghanistan (CNN) -- Thousands of bottl...   \n",
       "7  (CNN) -- Tour de France race director Christia...   \n",
       "8  (CNN) -- Hundreds filed by a casket on Sunday ...   \n",
       "9  Earlier this season I picked Thierry Henry as ...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Bishop John Folda, of North Dakota, is taking ...  \n",
       "1  China's top security official visited Afghanis...  \n",
       "2  The Virgin Group was founded by Richard Branso...  \n",
       "3  Two men filmed taking iPad from canoe rental o...  \n",
       "4  Muqtada al-Sadr has been in Iran since 2007 .\\...  \n",
       "5  Zhu Sanni, 23, had been left alone at home for...  \n",
       "6  Official: Bottles are almost exclusively from ...  \n",
       "7  The 2013 Tour de France will start from the Fr...  \n",
       "8  Wes Leonard collapsed after scoring a winning ...  \n",
       "9  Sportsmail columnist Martin Keown was honoured...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h3 style='text-align:center'>Example text from the dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:55:00.772968Z",
     "iopub.status.busy": "2025-04-05T15:55:00.772652Z",
     "iopub.status.idle": "2025-04-05T15:55:00.779459Z",
     "shell.execute_reply": "2025-04-05T15:55:00.778490Z",
     "shell.execute_reply.started": "2025-04-05T15:55:00.772940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:55:00.780915Z",
     "iopub.status.busy": "2025-04-05T15:55:00.780613Z",
     "iopub.status.idle": "2025-04-05T15:55:00.789888Z",
     "shell.execute_reply": "2025-04-05T15:55:00.789169Z",
     "shell.execute_reply.started": "2025-04-05T15:55:00.780887Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\\nHe contracted the infection through contaminated food in Italy .\\nChurch members in Fargo, Grand Forks and Jamestown could have been exposed .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['highlights'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='text-align:center;'><strong>Preprocessing data</strong></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align:center;'>\n",
    "The initial step involves converting the input and target text into sequences of tokens, which can be individual words or sub-word units. This is typically achieved through tokenization techniques. To ensure uniform input shapes for the model, the sequences are then padded with special tokens (e.g., &lt;PAD&gt;) to achieve equal lengths. Finally, to provide clear boundaries for the model, special start (&lt;START&gt;) and end (&lt;END&gt;) tokens are added to the beginning and end of the target sequences, respectively. This preprocessed data is then ready to be fed into the seq2seq model for training and inference.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T18:40:22.887355Z",
     "iopub.status.busy": "2025-04-10T18:40:22.887060Z",
     "iopub.status.idle": "2025-04-10T18:40:22.894027Z",
     "shell.execute_reply": "2025-04-10T18:40:22.893283Z",
     "shell.execute_reply.started": "2025-04-10T18:40:22.887335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X, y = np.array(train.iloc[:, 0:1]), np.array(train.iloc[:,1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T18:40:25.737696Z",
     "iopub.status.busy": "2025-04-10T18:40:25.736980Z",
     "iopub.status.idle": "2025-04-10T18:40:25.742111Z",
     "shell.execute_reply": "2025-04-10T18:40:25.741148Z",
     "shell.execute_reply.started": "2025-04-10T18:40:25.737672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X, y = X.reshape(X.shape[0]), y.reshape(y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Adding \"start\" and \"end\" token to the label datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T18:40:28.840592Z",
     "iopub.status.busy": "2025-04-10T18:40:28.839987Z",
     "iopub.status.idle": "2025-04-10T18:40:28.844491Z",
     "shell.execute_reply": "2025-04-10T18:40:28.843849Z",
     "shell.execute_reply.started": "2025-04-10T18:40:28.840571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "START = '<start>'\n",
    "END = '<end>'\n",
    "PAD = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T18:40:39.829535Z",
     "iopub.status.busy": "2025-04-10T18:40:39.828899Z",
     "iopub.status.idle": "2025-04-10T18:40:39.842865Z",
     "shell.execute_reply": "2025-04-10T18:40:39.842228Z",
     "shell.execute_reply.started": "2025-04-10T18:40:39.829509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y = [f\"{START} {text} {END}\" for text in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4 style='text-align:center;'>Taking out a few data points for infernce</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T18:40:33.529887Z",
     "iopub.status.busy": "2025-04-10T18:40:33.529288Z",
     "iopub.status.idle": "2025-04-10T18:40:33.534413Z",
     "shell.execute_reply": "2025-04-10T18:40:33.533693Z",
     "shell.execute_reply.started": "2025-04-10T18:40:33.529865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "size = -10\n",
    "X_test, y_test = X[size:], y[size:]\n",
    "X, y = X[:size], y[:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4 style='text-align:center;'>Preparing Tokenizer and finding vocabulary size</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:43:30.136818Z",
     "iopub.status.busy": "2025-04-06T07:43:30.136483Z",
     "iopub.status.idle": "2025-04-06T07:43:33.953980Z",
     "shell.execute_reply": "2025-04-06T07:43:33.953301Z",
     "shell.execute_reply.started": "2025-04-06T07:43:30.136787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "e_tk, d_tk = Tokenizer(), Tokenizer()\n",
    "e_tk.fit_on_texts(X)\n",
    "d_tk.fit_on_texts(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:43:46.424419Z",
     "iopub.status.busy": "2025-04-06T07:43:46.424058Z",
     "iopub.status.idle": "2025-04-06T07:43:46.428599Z",
     "shell.execute_reply": "2025-04-06T07:43:46.427648Z",
     "shell.execute_reply.started": "2025-04-06T07:43:46.424388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "start_id = d_tk.word_index.get(START.strip('<>'))\n",
    "end_id = d_tk.word_index.get(END.strip('<>'))\n",
    "pad_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:43:49.609215Z",
     "iopub.status.busy": "2025-04-06T07:43:49.608601Z",
     "iopub.status.idle": "2025-04-06T07:43:49.615067Z",
     "shell.execute_reply": "2025-04-06T07:43:49.614065Z",
     "shell.execute_reply.started": "2025-04-06T07:43:49.609150Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91127, 41940)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_vocab_size, out_vocab_size = len(e_tk.word_index) + 1, len(d_tk.word_index) + 1\n",
    "in_vocab_size, out_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4 style='text-align:center;'>Converting text to sequences, padding them and finalizing the three series (enc_inputs, dec_inputs, targets) <br> analogous to (X, dec_target_input, y)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:43:56.760335Z",
     "iopub.status.busy": "2025-04-06T07:43:56.759767Z",
     "iopub.status.idle": "2025-04-06T07:43:59.513525Z",
     "shell.execute_reply": "2025-04-06T07:43:59.512676Z",
     "shell.execute_reply.started": "2025-04-06T07:43:56.760301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "enc_inputs = e_tk.texts_to_sequences(X)\n",
    "targets = d_tk.texts_to_sequences(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:44:06.355618Z",
     "iopub.status.busy": "2025-04-06T07:44:06.355275Z",
     "iopub.status.idle": "2025-04-06T07:44:06.368591Z",
     "shell.execute_reply": "2025-04-06T07:44:06.367738Z",
     "shell.execute_reply.started": "2025-04-06T07:44:06.355588Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(329, 95)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_len = lambda x : max([len(seq) for seq in x])+1\n",
    "input_seq_len, output_seq_len = find_len(enc_inputs), find_len(targets)\n",
    "input_seq_len, output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:44:09.743454Z",
     "iopub.status.busy": "2025-04-06T07:44:09.742765Z",
     "iopub.status.idle": "2025-04-06T07:44:09.970912Z",
     "shell.execute_reply": "2025-04-06T07:44:09.969937Z",
     "shell.execute_reply.started": "2025-04-06T07:44:09.743420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "enc_inputs =np.array(pad_sequences(enc_inputs, padding='post', truncating='post', maxlen = input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:44:12.170927Z",
     "iopub.status.busy": "2025-04-06T07:44:12.170043Z",
     "iopub.status.idle": "2025-04-06T07:44:12.245857Z",
     "shell.execute_reply": "2025-04-06T07:44:12.244919Z",
     "shell.execute_reply.started": "2025-04-06T07:44:12.170886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "targets = pad_sequences(targets, padding='post', truncating='post', maxlen = output_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:44:15.314098Z",
     "iopub.status.busy": "2025-04-06T07:44:15.313548Z",
     "iopub.status.idle": "2025-04-06T07:44:15.324026Z",
     "shell.execute_reply": "2025-04-06T07:44:15.323138Z",
     "shell.execute_reply.started": "2025-04-06T07:44:15.314064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dec_inputs = np.array(targets[:, :-1])\n",
    "targets =  np.array(targets[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4 style='text-align:center;'>Dimensions of parameter</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:44:17.879617Z",
     "iopub.status.busy": "2025-04-06T07:44:17.879036Z",
     "iopub.status.idle": "2025-04-06T07:44:17.885197Z",
     "shell.execute_reply": "2025-04-06T07:44:17.884288Z",
     "shell.execute_reply.started": "2025-04-06T07:44:17.879580Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91127, 41940, 329, 95)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_vocab_size, out_vocab_size, input_seq_len, output_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "<h2 style='text-align:center;'><strong> \n",
    "    Attention Mechanism \n",
    "    </strong> (Bahdanau Attention)</h2>\n",
    "\n",
    "Bahdanau attention, also known as additive attention, is a mechanism designed to improve the performance of sequence-to-sequence models. It works by enabling the model to focus on specific parts of the input sequence when generating each part of the output sequence.\n",
    "\n",
    "<p>The decoder hidden state $s_{t}$ (query) at the $t^{th}$ timestep is passed to all encoder hidden states (keys : $h_{1}$, $h_{2}$,..., $h_{T}$) to calculate scores.\n",
    "The attention mechanism ensures that the decoder focuses on the most relevant parts of the input (as represented by the keys) when generating the next output token.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:32:14.765602Z",
     "iopub.status.busy": "2025-04-11T06:32:14.764816Z",
     "iopub.status.idle": "2025-04-11T06:32:14.771440Z",
     "shell.execute_reply": "2025-04-11T06:32:14.770621Z",
     "shell.execute_reply.started": "2025-04-11T06:32:14.765579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(L.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = L.Dense(units)\n",
    "        self.W2 = L.Dense(units)\n",
    "        self.V = L.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query = tf.expand_dims(query, axis = 1)                # (batch_size, 1, hidden_size)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query) + self.W2(values)))  # (batch_size, timesteps, 1)\n",
    "        attention_weight = tf.nn.softmax(score, axis = 1)      # (batch_size, timesteps, 1)\n",
    "        context = attention_weight*values                      # (batch_size, timesteps, hidden_size)\n",
    "        context_vector = tf.reduce_sum(context, axis = 1)      # (batch_size, hidden_size)\n",
    "        return context_vector, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "<h2 style='text-align:center;'><strong> \n",
    "    Model Definition \n",
    "    </strong></h2>\n",
    "\n",
    "+ Teacher Forcing is implemented in the  **train_step**  method where we use the actual target sequence as input to the decoder during training.\n",
    "+ The model uses separate Encoder and Decoder classes for better organization.\n",
    "+ A generate method is included for inference, which uses the model's own predictions rather than teacher forcing.\n",
    "+ The architecture uses LSTM cells, but you can easily modify it to use GRU or other RNN cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:32:22.238051Z",
     "iopub.status.busy": "2025-04-11T06:32:22.237287Z",
     "iopub.status.idle": "2025-04-11T06:32:22.244815Z",
     "shell.execute_reply": "2025-04-11T06:32:22.243860Z",
     "shell.execute_reply.started": "2025-04-11T06:32:22.238017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(L.Layer):\n",
    "    def __init__(self, in_vocab, embedding_dim, hidden_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = L.Embedding(in_vocab, embedding_dim)       # (batch_size, seq_length) -> (batch_size, seq_length, embedding_dim)\n",
    "        self.lstm = L.LSTM(hidden_units, return_sequences=True,return_state = True)   # (batch_size, seq_length, embedding_dim) -> (batch_size, hidden_units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embed(inputs)                               # (batch_size, seq_length, embeddign_dim)\n",
    "        enc_out, hidden_state, cell_state = self.lstm(x)     # O/P (batch_size, seq_len, hidden_size)\n",
    "        return enc_out, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:32:26.839719Z",
     "iopub.status.busy": "2025-04-11T06:32:26.839403Z",
     "iopub.status.idle": "2025-04-11T06:32:26.846692Z",
     "shell.execute_reply": "2025-04-11T06:32:26.845643Z",
     "shell.execute_reply.started": "2025-04-11T06:32:26.839696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(L.Layer):\n",
    "    def __init__(self, out_vocab, embedding_dim, hidden_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = L.Embedding(out_vocab, embedding_dim)     # (batch_size, seq_length) -> (batch_size, seq_length, embedding_dim)\n",
    "        self.lstm = L.LSTM(hidden_units, return_sequences = True, return_state = True)  # (batch_size, seq_length, embedding_dim) -> (batch_size, hidden_units)\n",
    "        self.dense = L.Dense(out_vocab, activation='softmax')  # (batch_size, seq_length, hidden_units) -> (batch_size, seq_length, out_vocab)\n",
    "        self.attention = BahdanauAttention(64)\n",
    "    \n",
    "    def call(self, inputs, hidden_state, cell_state, enc_output):\n",
    "        # input : (batch_size, 1)\n",
    "        x = self.embed(inputs)                                 # (batch_size, 1, embedding_dim)\n",
    "        states = [hidden_state, cell_state] \n",
    "        context, attention_weights = self.attention(query = hidden_state, values = enc_output)\n",
    "        dec_out, hidden_state, cell_state = self.lstm(x, initial_state=states)  # O/P : (batch_size, 1, hidden_units)\n",
    "        dec_out = tf.squeeze(dec_out, axis=1)                  # (batch_size, hidden_units)\n",
    "        inputs = tf.concat([context, dec_out], axis=-1)        # (batch_size, 1, embedding_dim + enc_units)\n",
    "        out = self.dense(inputs)                               # (batch_size, 1, out_vocab)\n",
    "        return out, hidden_state, cell_state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style='text-align:center;'><strong>Teacher Forcing</strong></h2>\n",
    "<p style='text-align:center;'>Instead of passing the output of the current timestep as the input to the next \n",
    "During training, teacher forcing provides the model with the ground truth (actual) output from the training data instead of feeding the model's own previous output as input. Teacher Forcing makes convergence faster during training especially in the starting epochs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:32:36.977073Z",
     "iopub.status.busy": "2025-04-11T06:32:36.976764Z",
     "iopub.status.idle": "2025-04-11T06:32:36.994917Z",
     "shell.execute_reply": "2025-04-11T06:32:36.994013Z",
     "shell.execute_reply.started": "2025-04-11T06:32:36.977051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"Custom\", name=\"Seq2Seq\")\n",
    "class Seq2Seq(M.Model):\n",
    "\n",
    "    def __init__(self, in_vocab, out_vocab, embedding_dim, hidden_units, end_token):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.in_vocab = in_vocab\n",
    "        self.out_vocab = out_vocab\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        self.encoder = Encoder(in_vocab, embedding_dim, hidden_units)\n",
    "        self.decoder = Decoder(out_vocab, embedding_dim, hidden_units)\n",
    "        self.end_token = end_token\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, inputs):\n",
    "        (enc_inputs, dec_inputs), targets = inputs         # (batch_size, seq_length)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_out, hidden_state, cell_state = self.encoder(enc_inputs)           # (batch_size, hidden_units)\n",
    "            seq_len = dec_inputs.shape[1]\n",
    "            dec_out = tf.TensorArray(tf.float32, seq_len)  # (batch_size, seq_len, target_vocab_size)\n",
    "            mask = tf.TensorArray(tf.bool, size=seq_len)\n",
    "            for timestep in tf.range(seq_len):\n",
    "                timestep_input = dec_inputs[:, timestep:timestep+1]       # (batch_size, 1)\n",
    "                timestep_output, hidden_state, cell_state = self.decoder(timestep_input, hidden_state, cell_state, enc_out)   # timestep_output -> # (batch_size, 1, hidden_units)\n",
    "                dec_out = dec_out.write(timestep, timestep_output)\n",
    "                is_end = tf.equal(targets[:, timestep], self.end_token)  # Creating mask based on whether end token is encountered\n",
    "                mask = mask.write(timestep, tf.logical_not(is_end))\n",
    "            dec_out = tf.transpose(dec_out.stack(), [1, 0, 2])\n",
    "            sequence_mask = tf.transpose(mask.stack(), [1, 0])\n",
    "            loss = self.compiled_loss(targets, dec_out, sample_weight=tf.cast(sequence_mask, tf.float32))   \n",
    "        variables = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        self.compiled_metrics.update_state(targets, dec_out) # Update metrics\n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_inputs, dec_inputs = inputs                       \n",
    "        enc_out, hidden_state, cell_state = self.encoder(enc_inputs)   # (batch_size, hidden_units)\n",
    "        seq_len = tf.shape(dec_inputs)[1]\n",
    "        dec_out = tf.TensorArray(tf.float32, seq_len)  # (batch_size, seq_len, target_vocab_size)\n",
    "        for timestep in tf.range(seq_len):\n",
    "            timestep_input = dec_inputs[:, timestep:timestep+1]       # (batch_size, 1)\n",
    "            timestep_output, hidden_state, cell_state = self.decoder(timestep_input, hidden_state, cell_state, enc_out)   # timestep_output -> # (batch_size, 1, hidden_units)\n",
    "            dec_out = dec_out.write(timestep, timestep_output)\n",
    "        return tf.transpose(dec_out.stack(), [1, 0, 2])\n",
    "    \n",
    "\n",
    "    def generate(self, enc_inputs, max_len, start, end):\n",
    "        enc_out, hidden_state, cell_state = self.encoder(enc_inputs)\n",
    "        dec_in = tf.expand_dims([start], 0)              # To get from int -> (1,1) tensor\n",
    "        result = []\n",
    "        for _ in range(max_len): \n",
    "            prediction_logits, hidden_state, cell_state = self.decoder(dec_in, hidden_state, cell_state, enc_out) # (1, 1, hidden_units)\n",
    "            prediction = tf.argmax(prediction_logits, axis=-1)        # return token ID (int)\n",
    "            if prediction == end:\n",
    "                break\n",
    "            result.append(prediction.numpy())\n",
    "            dec_in = tf.expand_dims(prediction, 0) \n",
    "        return result\n",
    "    def get_config(self):\n",
    "        config = super(Seq2Seq, self).get_config()\n",
    "        config.update({\n",
    "            'in_vocab': self.in_vocab,\n",
    "            'out_vocab': self.out_vocab,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'hidden_units': self.hidden_units,\n",
    "            'end_token': self.end_token  \n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        end_token = config.get('end_token', 0)  \n",
    "        return cls(\n",
    "            in_vocab=config['in_vocab'],\n",
    "            out_vocab=config['out_vocab'],\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            hidden_units=config['hidden_units'],\n",
    "            end_token=end_token\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "<h2 style='text-align:center;'><strong>Model Instance and Training</strong></h2>\n",
    "<p style='text-align:center;'>The model was trained over 40 epochs of training, hyperparameters are 512 embedding dimension and 512 LSTM units in both encoder and decoder.</p>\n",
    "<p style='text-align:center;'>This Notebook may not contain the model training output as it was saved and we again tried to train but due to resource exhaustion errors for bigger text size you can see training cells don't have output </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:55:08.835177Z",
     "iopub.status.busy": "2025-04-05T15:55:08.834870Z",
     "iopub.status.idle": "2025-04-05T15:55:09.722304Z",
     "shell.execute_reply": "2025-04-05T15:55:09.721529Z",
     "shell.execute_reply.started": "2025-04-05T15:55:08.835150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq(in_vocab=in_vocab_size, out_vocab=out_vocab_size, embedding_dim=1024, hidden_units=512, end_token=end_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:55:09.723505Z",
     "iopub.status.busy": "2025-04-05T15:55:09.723254Z",
     "iopub.status.idle": "2025-04-05T15:55:09.743818Z",
     "shell.execute_reply": "2025-04-05T15:55:09.742900Z",
     "shell.execute_reply.started": "2025-04-05T15:55:09.723479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:55:09.745667Z",
     "iopub.status.busy": "2025-04-05T15:55:09.745020Z",
     "iopub.status.idle": "2025-04-05T16:03:22.436190Z",
     "shell.execute_reply": "2025-04-05T16:03:22.435277Z",
     "shell.execute_reply.started": "2025-04-05T15:55:09.745635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:603: UserWarning: `model.compiled_loss()` is deprecated. Instead, use `model.compute_loss(x, y, y_pred, sample_weight)`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:578: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n",
      "```\n",
      "for metric in self.metrics:\n",
      "    metric.update_state(y, y_pred)\n",
      "```\n",
      "\n",
      "  return self._compiled_metrics_update_state(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 1s/step - accuracy: 0.5928 - loss: 2.3844e-05 - val_accuracy: 0.6292 - val_loss: 2.9843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x78dc2135e5f0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit((enc_inputs, dec_inputs), targets, batch_size=32, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:03:22.437684Z",
     "iopub.status.busy": "2025-04-05T16:03:22.437381Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 1s/step - accuracy: 0.6321 - loss: 2.3844e-05 - val_accuracy: 0.6478 - val_loss: 2.7503\n",
      "Epoch 2/20\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 1s/step - accuracy: 0.6496 - loss: 2.3844e-05 - val_accuracy: 0.6552 - val_loss: 2.6526\n",
      "Epoch 3/20\n",
      "\u001b[1m284/470\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2:58\u001b[0m 961ms/step - accuracy: 0.6636 - loss: 2.3844e-05"
     ]
    }
   ],
   "source": [
    "model.fit((enc_inputs, dec_inputs), targets, batch_size=32, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.fit((enc_inputs, dec_inputs), targets, batch_size=32, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.fit((enc_inputs, dec_inputs), targets, batch_size=32, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><strong><span style=\"font-size: 24px;\">Saving all required files after loading the model</span></strong></p>\n",
    "\n",
    "After training the model with attention and teacher forcing, the following components are saved to ensure reproducibility and smooth inference:\n",
    "\n",
    "- The trained **Keras model** is saved, which includes both the architecture and the learned weights.\n",
    "- **Encoder and decoder tokenizers** are saved using `pickle`, preserving the vocabulary and tokenization logic.\n",
    "- A **metadata dictionary** is saved containing:\n",
    "  - The word-to-index mapping (`word_dict`)\n",
    "  - Special tokens like `start_id` and `end_id`\n",
    "  - Sequence lengths for both input and output (`input_seq_len` and `output_seq_len`)\n",
    "\n",
    "These files together enable the model to be reloaded and used for prediction without the need for retraining or redefining preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save('Attention_Model_(teacher_forcing).keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"e_tk.pkl\", \"wb\") as f:\n",
    "    pickle.dump(e_tk, f)\n",
    "\n",
    "with open(\"d_tk.pkl\", \"wb\") as f:\n",
    "    pickle.dump(d_tk, f)\n",
    "\n",
    "metadata = {\n",
    "    \"word_dict\": word_dict,\n",
    "    \"start_id\": start_id,\n",
    "    \"end_id\": end_id,\n",
    "    \"input_seq_len\": input_seq_len,\n",
    "    \"output_seq_len\": output_seq_len\n",
    "}\n",
    "\n",
    "with open(\"metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "<h3 style='text-align:center;'><strong>Model Inference</strong></h3>\n",
    "<p style='text-align:center;'>\n",
    "With the required saved model and associated files (such as encoder/decoder tokenizers and metadata), the cells under this section can be executed to perform inference or evaluate the model on test data <strong>without retraining the model</strong>. This setup enables quick generation of summaries and computation of evaluation metrics like ROUGE directly.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:44:44.750566Z",
     "iopub.status.busy": "2025-04-06T07:44:44.750227Z",
     "iopub.status.idle": "2025-04-06T07:44:44.761106Z",
     "shell.execute_reply": "2025-04-06T07:44:44.760308Z",
     "shell.execute_reply.started": "2025-04-06T07:44:44.750537Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# word_dict = {v : k for k,v in d_tk.word_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><strong><span style=\"font-size: 24px;\">Loading the Trained Model for Inference</span></strong></p>\n",
    "\n",
    "The trained attention-based Seq2Seq model is loaded using Keras for performing inference on new data.\n",
    "\n",
    "- The model was saved after training and is now reloaded using `load_model()`.\n",
    "- Since a custom model class (`Seq2Seq`) was used during training, it must be specified in the `custom_objects` argument to ensure proper reconstruction.\n",
    "- This step restores the trained model's architecture and learned weights, making it ready for inference without retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:32:50.844940Z",
     "iopub.status.busy": "2025-04-11T06:32:50.844132Z",
     "iopub.status.idle": "2025-04-11T06:33:18.831640Z",
     "shell.execute_reply": "2025-04-11T06:33:18.830959Z",
     "shell.execute_reply.started": "2025-04-11T06:32:50.844913Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744353172.051433      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"/kaggle/input/model123/Attention_Model_(teacher_forcing).keras\", custom_objects={\"Seq2Seq\": Seq2Seq})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><strong><span style=\"font-size: 24px;\">Loading Tokenizers and Metadata for Inference</span></strong></p>\n",
    "\n",
    "To perform inference using the trained model, it's essential to load the same preprocessing components used during training:\n",
    "\n",
    "- **Encoder and decoder tokenizers** are loaded using `pickle`. These tokenizers preserve the vocabulary mappings required for encoding input sequences and decoding outputs.\n",
    "- A **metadata file** is also loaded, which contains:\n",
    "  - `word_dict`: Mapping of words to indices\n",
    "  - `start_id` and `end_id`: Special tokens used to denote the beginning and end of sequences\n",
    "  - `input_seq_len` and `output_seq_len`: Sequence lengths used during training\n",
    "\n",
    "Loading these components ensures that the input data is tokenized and padded exactly as it was during training, which is crucial for accurate inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:51.327490Z",
     "iopub.status.busy": "2025-04-11T06:35:51.327171Z",
     "iopub.status.idle": "2025-04-11T06:35:51.578156Z",
     "shell.execute_reply": "2025-04-11T06:35:51.577531Z",
     "shell.execute_reply.started": "2025-04-11T06:35:51.327468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load encoder and decoder tokenizers\n",
    "with open(\"/kaggle/input/pkl1234556/e_tk.pkl\", \"rb\") as f:\n",
    "    e_tk = pickle.load(f)\n",
    "\n",
    "with open(\"/kaggle/input/pkl1234556/d_tk.pkl\", \"rb\") as f:\n",
    "    d_tk = pickle.load(f)\n",
    "\n",
    "# Load metadata\n",
    "with open(\"/kaggle/input/pkl1234556/metadata.pkl\", \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "word_dict = metadata[\"word_dict\"]\n",
    "start_id = metadata[\"start_id\"]\n",
    "end_id = metadata[\"end_id\"]\n",
    "input_seq_len = metadata[\"input_seq_len\"]\n",
    "output_seq_len = metadata[\"output_seq_len\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><strong><span style=\"font-size: 24px;\">Generating Summaries Using the Trained Model</span></strong></p>\n",
    "\n",
    "This function performs **text summarization** using the trained attention-based Seq2Seq model. It takes raw input text and returns a generated summary by following these steps:\n",
    "\n",
    "- **Tokenization & Padding**:  \n",
    "  The input text is first tokenized using the encoder tokenizer and padded to match the input sequence length used during training.\n",
    "\n",
    "- **Prediction**:  \n",
    "  The padded input is passed to the model's `generate` method along with the maximum target length, `start_id`, and `end_id` to produce a sequence of output token IDs.\n",
    "\n",
    "- **Decoding**:  \n",
    "  The generated token IDs are converted back to words using the `word_dict`. Decoding stops upon encountering the `end_id`.\n",
    "\n",
    "- **Return Summary**:  \n",
    "  The decoded words are joined to form the final summarized text, which is returned by the function.\n",
    "\n",
    "This setup ensures that the model can generate summaries for new inputs using the same preprocessing pipeline as in training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:31.782601Z",
     "iopub.status.busy": "2025-04-11T06:35:31.782254Z",
     "iopub.status.idle": "2025-04-11T06:35:31.789506Z",
     "shell.execute_reply": "2025-04-11T06:35:31.788639Z",
     "shell.execute_reply.started": "2025-04-11T06:35:31.782576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def summarize_text(text, model, source_tokenizer, word_dict, \n",
    "                   start_id, end_id, source_max, target_max):\n",
    "    \n",
    "    # Tokenize and pad input\n",
    "    seq = source_tokenizer.texts_to_sequences([text])\n",
    "    seq = pad_sequences(seq, maxlen=source_max, padding='post')\n",
    "\n",
    "    # Generate predictions\n",
    "    model_output = model.generate(seq, target_max, start_id, end_id)\n",
    "\n",
    "    # Decode output tokens to words\n",
    "    output_text = []\n",
    "    for token_id in model_output:\n",
    "        if isinstance(token_id, (list, tuple, np.ndarray)):\n",
    "            token_id = int(token_id[0])\n",
    "        else:\n",
    "            token_id = int(token_id)\n",
    "\n",
    "        if token_id == end_id:\n",
    "            break\n",
    "\n",
    "        word = word_dict.get(token_id, '')\n",
    "        if word:\n",
    "            output_text.append(word)\n",
    "\n",
    "    return ' '.join(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference of a random summary to show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><strong><span style=\"font-size: 24px;\">Testing the summarize_text Function</span></strong></p>\n",
    "\n",
    "A sample input is used to test the `summarize_text` function and verify that it returns a valid summary.\n",
    "\n",
    "- The function is called with a test input and necessary components.\n",
    "- The generated summary is printed alongside the expected summary for a quick comparison.\n",
    "\n",
    "This simple check ensures the inference pipeline is working as intended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T18:46:52.196281Z",
     "iopub.status.busy": "2025-04-10T18:46:52.195900Z",
     "iopub.status.idle": "2025-04-10T18:46:56.474273Z",
     "shell.execute_reply": "2025-04-10T18:46:56.473345Z",
     "shell.execute_reply.started": "2025-04-10T18:46:52.196259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744310813.219351      95 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT TEXT:\n",
      "By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "bishop john folda of north dakota is taking time off after being diagnosed he contracted the infection through contaminated food in italy church members in fargo grand forks and jamestown could have been exposed to 50 years in extra time the study allows 50 500 people while he was exposed to bounce back in extra time he says he contracted the bishop after his neck that exposed to 50 50 million people were exposed to hospital he contracted the infection in fargo grand forks and jamestown could have been exposed to bounce back in extra\n",
      "\n",
      "EXPECTED SUMMARY:\n",
      " Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
      "He contracted the infection through contaminated food in Italy .\n",
      "Church members in Fargo, Grand Forks and Jamestown could have been exposed . \n"
     ]
    }
   ],
   "source": [
    "example_text = X[0]  \n",
    "summary = summarize_text(\n",
    "    text=example_text,\n",
    "    model=model,\n",
    "    source_tokenizer=e_tk,\n",
    "    word_dict=word_dict,\n",
    "    start_id=start_id,\n",
    "    end_id=end_id,\n",
    "    source_max=input_seq_len,\n",
    "    target_max=output_seq_len\n",
    ")\n",
    "\n",
    "print(\"\\nEXPECTED SUMMARY:\")\n",
    "print(y[0][7:-5])  # trimming <start> and <end> from actual summary if applicable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><strong><span style=\"font-size: 24px;\">Evaluating the Model Using ROUGE Score</span></strong></p>\n",
    "\n",
    "To evaluate the quality of generated summaries, the **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** metric is used.\n",
    "\n",
    "- The generated summaries are compared against the reference (expected) summaries.\n",
    "- ROUGE scores measure the **overlap of n-grams, word sequences, and word pairs** between the two.\n",
    "- Commonly used variants include **ROUGE-1**, **ROUGE-2**, and **ROUGE-L** for unigram, bigram, and longest common subsequence matches.\n",
    "\n",
    "This evaluation provides a quantitative measure of how well the model is performing on the summarization task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><strong><span style=\"font-size: 24px;\">Loading and Preprocessing Test Data</span></strong></p>\n",
    "\n",
    "The test dataset is loaded to evaluate the performance of the trained model.\n",
    "\n",
    "- The test data is **preprocessed using the same steps** as the training data to maintain consistency.\n",
    "- This includes tokenization, padding, and any necessary formatting.\n",
    "- Ensuring identical preprocessing helps produce reliable and comparable evaluation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:33:53.824670Z",
     "iopub.status.busy": "2025-04-11T06:33:53.823986Z",
     "iopub.status.idle": "2025-04-11T06:33:55.009739Z",
     "shell.execute_reply": "2025-04-11T06:33:55.008702Z",
     "shell.execute_reply.started": "2025-04-11T06:33:53.824644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><strong><span style=\"font-size: 24px;\">Filtering Test Data Based on Length Constraints</span></strong></p>\n",
    "\n",
    "Due to **Kaggle's GPU resource constraints**, unfiltered long sequences can lead to **resource exhaustion errors** during inference.\n",
    "\n",
    "- To prevent this, the test data is filtered similarly to the training data:\n",
    "  - Articles longer than `TEXT_SIZE` and summaries longer than `SUMM_SIZE` are excluded.\n",
    "- This keeps the input within safe bounds for GPU processing and ensures smooth evaluation.\n",
    "\n",
    "Applying these constraints helps avoid crashes while maintaining evaluation consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:33:56.331927Z",
     "iopub.status.busy": "2025-04-11T06:33:56.331619Z",
     "iopub.status.idle": "2025-04-11T06:33:56.353503Z",
     "shell.execute_reply": "2025-04-11T06:33:56.352750Z",
     "shell.execute_reply.started": "2025-04-11T06:33:56.331905Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "879"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test[test['article'].apply(lambda x: len(x)<TEXT_SIZE)]\n",
    "test = test[test['highlights'].apply(lambda x: len(x)<SUMM_SIZE)]\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:34:01.115437Z",
     "iopub.status.busy": "2025-04-11T06:34:01.115104Z",
     "iopub.status.idle": "2025-04-11T06:34:01.124509Z",
     "shell.execute_reply": "2025-04-11T06:34:01.123742Z",
     "shell.execute_reply.started": "2025-04-11T06:34:01.115409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = test.reset_index().drop(['index','id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:34:04.470458Z",
     "iopub.status.busy": "2025-04-11T06:34:04.470136Z",
     "iopub.status.idle": "2025-04-11T06:34:04.476515Z",
     "shell.execute_reply": "2025-04-11T06:34:04.475646Z",
     "shell.execute_reply.started": "2025-04-11T06:34:04.470438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X, y = np.array(test.iloc[:, 0:1]), np.array(test.iloc[:,1:2])\n",
    "X, y = X.reshape(X.shape[0]), y.reshape(y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:34:07.083111Z",
     "iopub.status.busy": "2025-04-11T06:34:07.082288Z",
     "iopub.status.idle": "2025-04-11T06:34:07.087615Z",
     "shell.execute_reply": "2025-04-11T06:34:07.086705Z",
     "shell.execute_reply.started": "2025-04-11T06:34:07.083085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "START = '<start>'\n",
    "END = '<end>'\n",
    "PAD = '<PAD>'\n",
    "y = [f\"{START} {text} {END}\" for text in y]\n",
    "X_test, y_test = X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><strong><span style=\"font-size: 24px;\">Evaluating Model Using ROUGE Scores</span></strong></p>\n",
    "\n",
    "To evaluate the summarization quality of the trained model, **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** metrics are used:\n",
    "\n",
    "- ROUGE-1, ROUGE-2, and ROUGE-L scores are computed to assess the overlap between generated summaries and reference summaries.\n",
    "- The evaluation is performed in two phases:\n",
    "  1. **Initial Evaluation:** On a small subset (first 100 samples) for quick inspection and debugging.\n",
    "  2. **Full Evaluation:** On the entire test set to get a complete performance picture.\n",
    "- Each generated summary is compared against the ground truth using a **stemmer-based ROUGE scorer**, and average F1 scores are calculated.\n",
    "\n",
    "These metrics provide insight into the model's ability to retain important content from the input text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T19:50:58.265202Z",
     "iopub.status.busy": "2025-04-10T19:50:58.264490Z",
     "iopub.status.idle": "2025-04-10T19:53:46.953481Z",
     "shell.execute_reply": "2025-04-10T19:53:46.952861Z",
     "shell.execute_reply.started": "2025-04-10T19:50:58.265174Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing article 1/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744314660.054336      93 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing article 2/100...\n",
      "🔄 Processing article 3/100...\n",
      "🔄 Processing article 4/100...\n",
      "🔄 Processing article 5/100...\n",
      "🔄 Processing article 6/100...\n",
      "🔄 Processing article 7/100...\n",
      "🔄 Processing article 8/100...\n",
      "🔄 Processing article 9/100...\n",
      "🔄 Processing article 10/100...\n",
      "🔄 Processing article 11/100...\n",
      "🔄 Processing article 12/100...\n",
      "🔄 Processing article 13/100...\n",
      "🔄 Processing article 14/100...\n",
      "🔄 Processing article 15/100...\n",
      "🔄 Processing article 16/100...\n",
      "🔄 Processing article 17/100...\n",
      "🔄 Processing article 18/100...\n",
      "🔄 Processing article 19/100...\n",
      "🔄 Processing article 20/100...\n",
      "🔄 Processing article 21/100...\n",
      "🔄 Processing article 22/100...\n",
      "🔄 Processing article 23/100...\n",
      "🔄 Processing article 24/100...\n",
      "🔄 Processing article 25/100...\n",
      "🔄 Processing article 26/100...\n",
      "🔄 Processing article 27/100...\n",
      "🔄 Processing article 28/100...\n",
      "🔄 Processing article 29/100...\n",
      "🔄 Processing article 30/100...\n",
      "🔄 Processing article 31/100...\n",
      "🔄 Processing article 32/100...\n",
      "🔄 Processing article 33/100...\n",
      "🔄 Processing article 34/100...\n",
      "🔄 Processing article 35/100...\n",
      "🔄 Processing article 36/100...\n",
      "🔄 Processing article 37/100...\n",
      "🔄 Processing article 38/100...\n",
      "🔄 Processing article 39/100...\n",
      "🔄 Processing article 40/100...\n",
      "🔄 Processing article 41/100...\n",
      "🔄 Processing article 42/100...\n",
      "🔄 Processing article 43/100...\n",
      "🔄 Processing article 44/100...\n",
      "🔄 Processing article 45/100...\n",
      "🔄 Processing article 46/100...\n",
      "🔄 Processing article 47/100...\n",
      "🔄 Processing article 48/100...\n",
      "🔄 Processing article 49/100...\n",
      "🔄 Processing article 50/100...\n",
      "🔄 Processing article 51/100...\n",
      "🔄 Processing article 52/100...\n",
      "🔄 Processing article 53/100...\n",
      "🔄 Processing article 54/100...\n",
      "🔄 Processing article 55/100...\n",
      "🔄 Processing article 56/100...\n",
      "🔄 Processing article 57/100...\n",
      "🔄 Processing article 58/100...\n",
      "🔄 Processing article 59/100...\n",
      "🔄 Processing article 60/100...\n",
      "🔄 Processing article 61/100...\n",
      "🔄 Processing article 62/100...\n",
      "🔄 Processing article 63/100...\n",
      "🔄 Processing article 64/100...\n",
      "🔄 Processing article 65/100...\n",
      "🔄 Processing article 66/100...\n",
      "🔄 Processing article 67/100...\n",
      "🔄 Processing article 68/100...\n",
      "🔄 Processing article 69/100...\n",
      "🔄 Processing article 70/100...\n",
      "🔄 Processing article 71/100...\n",
      "🔄 Processing article 72/100...\n",
      "🔄 Processing article 73/100...\n",
      "🔄 Processing article 74/100...\n",
      "🔄 Processing article 75/100...\n",
      "🔄 Processing article 76/100...\n",
      "🔄 Processing article 77/100...\n",
      "🔄 Processing article 78/100...\n",
      "🔄 Processing article 79/100...\n",
      "🔄 Processing article 80/100...\n",
      "🔄 Processing article 81/100...\n",
      "🔄 Processing article 82/100...\n",
      "🔄 Processing article 83/100...\n",
      "🔄 Processing article 84/100...\n",
      "🔄 Processing article 85/100...\n",
      "🔄 Processing article 86/100...\n",
      "🔄 Processing article 87/100...\n",
      "🔄 Processing article 88/100...\n",
      "🔄 Processing article 89/100...\n",
      "🔄 Processing article 90/100...\n",
      "🔄 Processing article 91/100...\n",
      "🔄 Processing article 92/100...\n",
      "🔄 Processing article 93/100...\n",
      "🔄 Processing article 94/100...\n",
      "🔄 Processing article 95/100...\n",
      "🔄 Processing article 96/100...\n",
      "🔄 Processing article 97/100...\n",
      "🔄 Processing article 98/100...\n",
      "🔄 Processing article 99/100...\n",
      "🔄 Processing article 100/100...\n",
      "\n",
      "📊 AVERAGE ROUGE F1 SCORES OVER FIRST 100 SAMPLES:\n",
      "ROUGE1: 0.1952\n",
      "ROUGE2: 0.0322\n",
      "ROUGEL: 0.1251\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Use only first 100 samples\n",
    "X_subset = X[:100]\n",
    "y_subset = y[:100]\n",
    "\n",
    "# Use last 10 from the subset\n",
    "X_test = X_subset\n",
    "y_test = y_subset\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "avg_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "# Generate summaries and calculate scores\n",
    "for i in range(len(X_test)):\n",
    "    print(f\"🔄 Processing article {i + 1}/{len(X_test)}...\")\n",
    "\n",
    "    summary = summarize_text(\n",
    "        text=X_test[i],\n",
    "        model=model,\n",
    "        source_tokenizer=e_tk,\n",
    "        word_dict=word_dict,\n",
    "        start_id=start_id,\n",
    "        end_id=end_id,\n",
    "        source_max=input_seq_len,\n",
    "        target_max=output_seq_len\n",
    "    )\n",
    "    \n",
    "    scores = scorer.score(y_test[i], summary)\n",
    "    for key in scores:\n",
    "        avg_scores[key].append(scores[key].fmeasure)\n",
    "\n",
    "# Compute and print average F1 scores\n",
    "print(\"\\n📊 AVERAGE ROUGE F1 SCORES OVER FIRST 100 SAMPLES:\")\n",
    "for key in avg_scores:\n",
    "    mean_f1 = sum(avg_scores[key]) / len(avg_scores[key])\n",
    "    print(f\"{key.upper()}: {mean_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:57.615154Z",
     "iopub.status.busy": "2025-04-11T06:35:57.614436Z",
     "iopub.status.idle": "2025-04-11T07:04:51.822888Z",
     "shell.execute_reply": "2025-04-11T07:04:51.822184Z",
     "shell.execute_reply.started": "2025-04-11T06:35:57.615132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing article 1/879...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744353358.586204      89 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing article 2/879...\n",
      "🔄 Processing article 3/879...\n",
      "🔄 Processing article 4/879...\n",
      "🔄 Processing article 5/879...\n",
      "🔄 Processing article 6/879...\n",
      "🔄 Processing article 7/879...\n",
      "🔄 Processing article 8/879...\n",
      "🔄 Processing article 9/879...\n",
      "🔄 Processing article 10/879...\n",
      "🔄 Processing article 11/879...\n",
      "🔄 Processing article 12/879...\n",
      "🔄 Processing article 13/879...\n",
      "🔄 Processing article 14/879...\n",
      "🔄 Processing article 15/879...\n",
      "🔄 Processing article 16/879...\n",
      "🔄 Processing article 17/879...\n",
      "🔄 Processing article 18/879...\n",
      "🔄 Processing article 19/879...\n",
      "🔄 Processing article 20/879...\n",
      "🔄 Processing article 21/879...\n",
      "🔄 Processing article 22/879...\n",
      "🔄 Processing article 23/879...\n",
      "🔄 Processing article 24/879...\n",
      "🔄 Processing article 25/879...\n",
      "🔄 Processing article 26/879...\n",
      "🔄 Processing article 27/879...\n",
      "🔄 Processing article 28/879...\n",
      "🔄 Processing article 29/879...\n",
      "🔄 Processing article 30/879...\n",
      "🔄 Processing article 31/879...\n",
      "🔄 Processing article 32/879...\n",
      "🔄 Processing article 33/879...\n",
      "🔄 Processing article 34/879...\n",
      "🔄 Processing article 35/879...\n",
      "🔄 Processing article 36/879...\n",
      "🔄 Processing article 37/879...\n",
      "🔄 Processing article 38/879...\n",
      "🔄 Processing article 39/879...\n",
      "🔄 Processing article 40/879...\n",
      "🔄 Processing article 41/879...\n",
      "🔄 Processing article 42/879...\n",
      "🔄 Processing article 43/879...\n",
      "🔄 Processing article 44/879...\n",
      "🔄 Processing article 45/879...\n",
      "🔄 Processing article 46/879...\n",
      "🔄 Processing article 47/879...\n",
      "🔄 Processing article 48/879...\n",
      "🔄 Processing article 49/879...\n",
      "🔄 Processing article 50/879...\n",
      "🔄 Processing article 51/879...\n",
      "🔄 Processing article 52/879...\n",
      "🔄 Processing article 53/879...\n",
      "🔄 Processing article 54/879...\n",
      "🔄 Processing article 55/879...\n",
      "🔄 Processing article 56/879...\n",
      "🔄 Processing article 57/879...\n",
      "🔄 Processing article 58/879...\n",
      "🔄 Processing article 59/879...\n",
      "🔄 Processing article 60/879...\n",
      "🔄 Processing article 61/879...\n",
      "🔄 Processing article 62/879...\n",
      "🔄 Processing article 63/879...\n",
      "🔄 Processing article 64/879...\n",
      "🔄 Processing article 65/879...\n",
      "🔄 Processing article 66/879...\n",
      "🔄 Processing article 67/879...\n",
      "🔄 Processing article 68/879...\n",
      "🔄 Processing article 69/879...\n",
      "🔄 Processing article 70/879...\n",
      "🔄 Processing article 71/879...\n",
      "🔄 Processing article 72/879...\n",
      "🔄 Processing article 73/879...\n",
      "🔄 Processing article 74/879...\n",
      "🔄 Processing article 75/879...\n",
      "🔄 Processing article 76/879...\n",
      "🔄 Processing article 77/879...\n",
      "🔄 Processing article 78/879...\n",
      "🔄 Processing article 79/879...\n",
      "🔄 Processing article 80/879...\n",
      "🔄 Processing article 81/879...\n",
      "🔄 Processing article 82/879...\n",
      "🔄 Processing article 83/879...\n",
      "🔄 Processing article 84/879...\n",
      "🔄 Processing article 85/879...\n",
      "🔄 Processing article 86/879...\n",
      "🔄 Processing article 87/879...\n",
      "🔄 Processing article 88/879...\n",
      "🔄 Processing article 89/879...\n",
      "🔄 Processing article 90/879...\n",
      "🔄 Processing article 91/879...\n",
      "🔄 Processing article 92/879...\n",
      "🔄 Processing article 93/879...\n",
      "🔄 Processing article 94/879...\n",
      "🔄 Processing article 95/879...\n",
      "🔄 Processing article 96/879...\n",
      "🔄 Processing article 97/879...\n",
      "🔄 Processing article 98/879...\n",
      "🔄 Processing article 99/879...\n",
      "🔄 Processing article 100/879...\n",
      "🔄 Processing article 101/879...\n",
      "🔄 Processing article 102/879...\n",
      "🔄 Processing article 103/879...\n",
      "🔄 Processing article 104/879...\n",
      "🔄 Processing article 105/879...\n",
      "🔄 Processing article 106/879...\n",
      "🔄 Processing article 107/879...\n",
      "🔄 Processing article 108/879...\n",
      "🔄 Processing article 109/879...\n",
      "🔄 Processing article 110/879...\n",
      "🔄 Processing article 111/879...\n",
      "🔄 Processing article 112/879...\n",
      "🔄 Processing article 113/879...\n",
      "🔄 Processing article 114/879...\n",
      "🔄 Processing article 115/879...\n",
      "🔄 Processing article 116/879...\n",
      "🔄 Processing article 117/879...\n",
      "🔄 Processing article 118/879...\n",
      "🔄 Processing article 119/879...\n",
      "🔄 Processing article 120/879...\n",
      "🔄 Processing article 121/879...\n",
      "🔄 Processing article 122/879...\n",
      "🔄 Processing article 123/879...\n",
      "🔄 Processing article 124/879...\n",
      "🔄 Processing article 125/879...\n",
      "🔄 Processing article 126/879...\n",
      "🔄 Processing article 127/879...\n",
      "🔄 Processing article 128/879...\n",
      "🔄 Processing article 129/879...\n",
      "🔄 Processing article 130/879...\n",
      "🔄 Processing article 131/879...\n",
      "🔄 Processing article 132/879...\n",
      "🔄 Processing article 133/879...\n",
      "🔄 Processing article 134/879...\n",
      "🔄 Processing article 135/879...\n",
      "🔄 Processing article 136/879...\n",
      "🔄 Processing article 137/879...\n",
      "🔄 Processing article 138/879...\n",
      "🔄 Processing article 139/879...\n",
      "🔄 Processing article 140/879...\n",
      "🔄 Processing article 141/879...\n",
      "🔄 Processing article 142/879...\n",
      "🔄 Processing article 143/879...\n",
      "🔄 Processing article 144/879...\n",
      "🔄 Processing article 145/879...\n",
      "🔄 Processing article 146/879...\n",
      "🔄 Processing article 147/879...\n",
      "🔄 Processing article 148/879...\n",
      "🔄 Processing article 149/879...\n",
      "🔄 Processing article 150/879...\n",
      "🔄 Processing article 151/879...\n",
      "🔄 Processing article 152/879...\n",
      "🔄 Processing article 153/879...\n",
      "🔄 Processing article 154/879...\n",
      "🔄 Processing article 155/879...\n",
      "🔄 Processing article 156/879...\n",
      "🔄 Processing article 157/879...\n",
      "🔄 Processing article 158/879...\n",
      "🔄 Processing article 159/879...\n",
      "🔄 Processing article 160/879...\n",
      "🔄 Processing article 161/879...\n",
      "🔄 Processing article 162/879...\n",
      "🔄 Processing article 163/879...\n",
      "🔄 Processing article 164/879...\n",
      "🔄 Processing article 165/879...\n",
      "🔄 Processing article 166/879...\n",
      "🔄 Processing article 167/879...\n",
      "🔄 Processing article 168/879...\n",
      "🔄 Processing article 169/879...\n",
      "🔄 Processing article 170/879...\n",
      "🔄 Processing article 171/879...\n",
      "🔄 Processing article 172/879...\n",
      "🔄 Processing article 173/879...\n",
      "🔄 Processing article 174/879...\n",
      "🔄 Processing article 175/879...\n",
      "🔄 Processing article 176/879...\n",
      "🔄 Processing article 177/879...\n",
      "🔄 Processing article 178/879...\n",
      "🔄 Processing article 179/879...\n",
      "🔄 Processing article 180/879...\n",
      "🔄 Processing article 181/879...\n",
      "🔄 Processing article 182/879...\n",
      "🔄 Processing article 183/879...\n",
      "🔄 Processing article 184/879...\n",
      "🔄 Processing article 185/879...\n",
      "🔄 Processing article 186/879...\n",
      "🔄 Processing article 187/879...\n",
      "🔄 Processing article 188/879...\n",
      "🔄 Processing article 189/879...\n",
      "🔄 Processing article 190/879...\n",
      "🔄 Processing article 191/879...\n",
      "🔄 Processing article 192/879...\n",
      "🔄 Processing article 193/879...\n",
      "🔄 Processing article 194/879...\n",
      "🔄 Processing article 195/879...\n",
      "🔄 Processing article 196/879...\n",
      "🔄 Processing article 197/879...\n",
      "🔄 Processing article 198/879...\n",
      "🔄 Processing article 199/879...\n",
      "🔄 Processing article 200/879...\n",
      "🔄 Processing article 201/879...\n",
      "🔄 Processing article 202/879...\n",
      "🔄 Processing article 203/879...\n",
      "🔄 Processing article 204/879...\n",
      "🔄 Processing article 205/879...\n",
      "🔄 Processing article 206/879...\n",
      "🔄 Processing article 207/879...\n",
      "🔄 Processing article 208/879...\n",
      "🔄 Processing article 209/879...\n",
      "🔄 Processing article 210/879...\n",
      "🔄 Processing article 211/879...\n",
      "🔄 Processing article 212/879...\n",
      "🔄 Processing article 213/879...\n",
      "🔄 Processing article 214/879...\n",
      "🔄 Processing article 215/879...\n",
      "🔄 Processing article 216/879...\n",
      "🔄 Processing article 217/879...\n",
      "🔄 Processing article 218/879...\n",
      "🔄 Processing article 219/879...\n",
      "🔄 Processing article 220/879...\n",
      "🔄 Processing article 221/879...\n",
      "🔄 Processing article 222/879...\n",
      "🔄 Processing article 223/879...\n",
      "🔄 Processing article 224/879...\n",
      "🔄 Processing article 225/879...\n",
      "🔄 Processing article 226/879...\n",
      "🔄 Processing article 227/879...\n",
      "🔄 Processing article 228/879...\n",
      "🔄 Processing article 229/879...\n",
      "🔄 Processing article 230/879...\n",
      "🔄 Processing article 231/879...\n",
      "🔄 Processing article 232/879...\n",
      "🔄 Processing article 233/879...\n",
      "🔄 Processing article 234/879...\n",
      "🔄 Processing article 235/879...\n",
      "🔄 Processing article 236/879...\n",
      "🔄 Processing article 237/879...\n",
      "🔄 Processing article 238/879...\n",
      "🔄 Processing article 239/879...\n",
      "🔄 Processing article 240/879...\n",
      "🔄 Processing article 241/879...\n",
      "🔄 Processing article 242/879...\n",
      "🔄 Processing article 243/879...\n",
      "🔄 Processing article 244/879...\n",
      "🔄 Processing article 245/879...\n",
      "🔄 Processing article 246/879...\n",
      "🔄 Processing article 247/879...\n",
      "🔄 Processing article 248/879...\n",
      "🔄 Processing article 249/879...\n",
      "🔄 Processing article 250/879...\n",
      "🔄 Processing article 251/879...\n",
      "🔄 Processing article 252/879...\n",
      "🔄 Processing article 253/879...\n",
      "🔄 Processing article 254/879...\n",
      "🔄 Processing article 255/879...\n",
      "🔄 Processing article 256/879...\n",
      "🔄 Processing article 257/879...\n",
      "🔄 Processing article 258/879...\n",
      "🔄 Processing article 259/879...\n",
      "🔄 Processing article 260/879...\n",
      "🔄 Processing article 261/879...\n",
      "🔄 Processing article 262/879...\n",
      "🔄 Processing article 263/879...\n",
      "🔄 Processing article 264/879...\n",
      "🔄 Processing article 265/879...\n",
      "🔄 Processing article 266/879...\n",
      "🔄 Processing article 267/879...\n",
      "🔄 Processing article 268/879...\n",
      "🔄 Processing article 269/879...\n",
      "🔄 Processing article 270/879...\n",
      "🔄 Processing article 271/879...\n",
      "🔄 Processing article 272/879...\n",
      "🔄 Processing article 273/879...\n",
      "🔄 Processing article 274/879...\n",
      "🔄 Processing article 275/879...\n",
      "🔄 Processing article 276/879...\n",
      "🔄 Processing article 277/879...\n",
      "🔄 Processing article 278/879...\n",
      "🔄 Processing article 279/879...\n",
      "🔄 Processing article 280/879...\n",
      "🔄 Processing article 281/879...\n",
      "🔄 Processing article 282/879...\n",
      "🔄 Processing article 283/879...\n",
      "🔄 Processing article 284/879...\n",
      "🔄 Processing article 285/879...\n",
      "🔄 Processing article 286/879...\n",
      "🔄 Processing article 287/879...\n",
      "🔄 Processing article 288/879...\n",
      "🔄 Processing article 289/879...\n",
      "🔄 Processing article 290/879...\n",
      "🔄 Processing article 291/879...\n",
      "🔄 Processing article 292/879...\n",
      "🔄 Processing article 293/879...\n",
      "🔄 Processing article 294/879...\n",
      "🔄 Processing article 295/879...\n",
      "🔄 Processing article 296/879...\n",
      "🔄 Processing article 297/879...\n",
      "🔄 Processing article 298/879...\n",
      "🔄 Processing article 299/879...\n",
      "🔄 Processing article 300/879...\n",
      "🔄 Processing article 301/879...\n",
      "🔄 Processing article 302/879...\n",
      "🔄 Processing article 303/879...\n",
      "🔄 Processing article 304/879...\n",
      "🔄 Processing article 305/879...\n",
      "🔄 Processing article 306/879...\n",
      "🔄 Processing article 307/879...\n",
      "🔄 Processing article 308/879...\n",
      "🔄 Processing article 309/879...\n",
      "🔄 Processing article 310/879...\n",
      "🔄 Processing article 311/879...\n",
      "🔄 Processing article 312/879...\n",
      "🔄 Processing article 313/879...\n",
      "🔄 Processing article 314/879...\n",
      "🔄 Processing article 315/879...\n",
      "🔄 Processing article 316/879...\n",
      "🔄 Processing article 317/879...\n",
      "🔄 Processing article 318/879...\n",
      "🔄 Processing article 319/879...\n",
      "🔄 Processing article 320/879...\n",
      "🔄 Processing article 321/879...\n",
      "🔄 Processing article 322/879...\n",
      "🔄 Processing article 323/879...\n",
      "🔄 Processing article 324/879...\n",
      "🔄 Processing article 325/879...\n",
      "🔄 Processing article 326/879...\n",
      "🔄 Processing article 327/879...\n",
      "🔄 Processing article 328/879...\n",
      "🔄 Processing article 329/879...\n",
      "🔄 Processing article 330/879...\n",
      "🔄 Processing article 331/879...\n",
      "🔄 Processing article 332/879...\n",
      "🔄 Processing article 333/879...\n",
      "🔄 Processing article 334/879...\n",
      "🔄 Processing article 335/879...\n",
      "🔄 Processing article 336/879...\n",
      "🔄 Processing article 337/879...\n",
      "🔄 Processing article 338/879...\n",
      "🔄 Processing article 339/879...\n",
      "🔄 Processing article 340/879...\n",
      "🔄 Processing article 341/879...\n",
      "🔄 Processing article 342/879...\n",
      "🔄 Processing article 343/879...\n",
      "🔄 Processing article 344/879...\n",
      "🔄 Processing article 345/879...\n",
      "🔄 Processing article 346/879...\n",
      "🔄 Processing article 347/879...\n",
      "🔄 Processing article 348/879...\n",
      "🔄 Processing article 349/879...\n",
      "🔄 Processing article 350/879...\n",
      "🔄 Processing article 351/879...\n",
      "🔄 Processing article 352/879...\n",
      "🔄 Processing article 353/879...\n",
      "🔄 Processing article 354/879...\n",
      "🔄 Processing article 355/879...\n",
      "🔄 Processing article 356/879...\n",
      "🔄 Processing article 357/879...\n",
      "🔄 Processing article 358/879...\n",
      "🔄 Processing article 359/879...\n",
      "🔄 Processing article 360/879...\n",
      "🔄 Processing article 361/879...\n",
      "🔄 Processing article 362/879...\n",
      "🔄 Processing article 363/879...\n",
      "🔄 Processing article 364/879...\n",
      "🔄 Processing article 365/879...\n",
      "🔄 Processing article 366/879...\n",
      "🔄 Processing article 367/879...\n",
      "🔄 Processing article 368/879...\n",
      "🔄 Processing article 369/879...\n",
      "🔄 Processing article 370/879...\n",
      "🔄 Processing article 371/879...\n",
      "🔄 Processing article 372/879...\n",
      "🔄 Processing article 373/879...\n",
      "🔄 Processing article 374/879...\n",
      "🔄 Processing article 375/879...\n",
      "🔄 Processing article 376/879...\n",
      "🔄 Processing article 377/879...\n",
      "🔄 Processing article 378/879...\n",
      "🔄 Processing article 379/879...\n",
      "🔄 Processing article 380/879...\n",
      "🔄 Processing article 381/879...\n",
      "🔄 Processing article 382/879...\n",
      "🔄 Processing article 383/879...\n",
      "🔄 Processing article 384/879...\n",
      "🔄 Processing article 385/879...\n",
      "🔄 Processing article 386/879...\n",
      "🔄 Processing article 387/879...\n",
      "🔄 Processing article 388/879...\n",
      "🔄 Processing article 389/879...\n",
      "🔄 Processing article 390/879...\n",
      "🔄 Processing article 391/879...\n",
      "🔄 Processing article 392/879...\n",
      "🔄 Processing article 393/879...\n",
      "🔄 Processing article 394/879...\n",
      "🔄 Processing article 395/879...\n",
      "🔄 Processing article 396/879...\n",
      "🔄 Processing article 397/879...\n",
      "🔄 Processing article 398/879...\n",
      "🔄 Processing article 399/879...\n",
      "🔄 Processing article 400/879...\n",
      "🔄 Processing article 401/879...\n",
      "🔄 Processing article 402/879...\n",
      "🔄 Processing article 403/879...\n",
      "🔄 Processing article 404/879...\n",
      "🔄 Processing article 405/879...\n",
      "🔄 Processing article 406/879...\n",
      "🔄 Processing article 407/879...\n",
      "🔄 Processing article 408/879...\n",
      "🔄 Processing article 409/879...\n",
      "🔄 Processing article 410/879...\n",
      "🔄 Processing article 411/879...\n",
      "🔄 Processing article 412/879...\n",
      "🔄 Processing article 413/879...\n",
      "🔄 Processing article 414/879...\n",
      "🔄 Processing article 415/879...\n",
      "🔄 Processing article 416/879...\n",
      "🔄 Processing article 417/879...\n",
      "🔄 Processing article 418/879...\n",
      "🔄 Processing article 419/879...\n",
      "🔄 Processing article 420/879...\n",
      "🔄 Processing article 421/879...\n",
      "🔄 Processing article 422/879...\n",
      "🔄 Processing article 423/879...\n",
      "🔄 Processing article 424/879...\n",
      "🔄 Processing article 425/879...\n",
      "🔄 Processing article 426/879...\n",
      "🔄 Processing article 427/879...\n",
      "🔄 Processing article 428/879...\n",
      "🔄 Processing article 429/879...\n",
      "🔄 Processing article 430/879...\n",
      "🔄 Processing article 431/879...\n",
      "🔄 Processing article 432/879...\n",
      "🔄 Processing article 433/879...\n",
      "🔄 Processing article 434/879...\n",
      "🔄 Processing article 435/879...\n",
      "🔄 Processing article 436/879...\n",
      "🔄 Processing article 437/879...\n",
      "🔄 Processing article 438/879...\n",
      "🔄 Processing article 439/879...\n",
      "🔄 Processing article 440/879...\n",
      "🔄 Processing article 441/879...\n",
      "🔄 Processing article 442/879...\n",
      "🔄 Processing article 443/879...\n",
      "🔄 Processing article 444/879...\n",
      "🔄 Processing article 445/879...\n",
      "🔄 Processing article 446/879...\n",
      "🔄 Processing article 447/879...\n",
      "🔄 Processing article 448/879...\n",
      "🔄 Processing article 449/879...\n",
      "🔄 Processing article 450/879...\n",
      "🔄 Processing article 451/879...\n",
      "🔄 Processing article 452/879...\n",
      "🔄 Processing article 453/879...\n",
      "🔄 Processing article 454/879...\n",
      "🔄 Processing article 455/879...\n",
      "🔄 Processing article 456/879...\n",
      "🔄 Processing article 457/879...\n",
      "🔄 Processing article 458/879...\n",
      "🔄 Processing article 459/879...\n",
      "🔄 Processing article 460/879...\n",
      "🔄 Processing article 461/879...\n",
      "🔄 Processing article 462/879...\n",
      "🔄 Processing article 463/879...\n",
      "🔄 Processing article 464/879...\n",
      "🔄 Processing article 465/879...\n",
      "🔄 Processing article 466/879...\n",
      "🔄 Processing article 467/879...\n",
      "🔄 Processing article 468/879...\n",
      "🔄 Processing article 469/879...\n",
      "🔄 Processing article 470/879...\n",
      "🔄 Processing article 471/879...\n",
      "🔄 Processing article 472/879...\n",
      "🔄 Processing article 473/879...\n",
      "🔄 Processing article 474/879...\n",
      "🔄 Processing article 475/879...\n",
      "🔄 Processing article 476/879...\n",
      "🔄 Processing article 477/879...\n",
      "🔄 Processing article 478/879...\n",
      "🔄 Processing article 479/879...\n",
      "🔄 Processing article 480/879...\n",
      "🔄 Processing article 481/879...\n",
      "🔄 Processing article 482/879...\n",
      "🔄 Processing article 483/879...\n",
      "🔄 Processing article 484/879...\n",
      "🔄 Processing article 485/879...\n",
      "🔄 Processing article 486/879...\n",
      "🔄 Processing article 487/879...\n",
      "🔄 Processing article 488/879...\n",
      "🔄 Processing article 489/879...\n",
      "🔄 Processing article 490/879...\n",
      "🔄 Processing article 491/879...\n",
      "🔄 Processing article 492/879...\n",
      "🔄 Processing article 493/879...\n",
      "🔄 Processing article 494/879...\n",
      "🔄 Processing article 495/879...\n",
      "🔄 Processing article 496/879...\n",
      "🔄 Processing article 497/879...\n",
      "🔄 Processing article 498/879...\n",
      "🔄 Processing article 499/879...\n",
      "🔄 Processing article 500/879...\n",
      "🔄 Processing article 501/879...\n",
      "🔄 Processing article 502/879...\n",
      "🔄 Processing article 503/879...\n",
      "🔄 Processing article 504/879...\n",
      "🔄 Processing article 505/879...\n",
      "🔄 Processing article 506/879...\n",
      "🔄 Processing article 507/879...\n",
      "🔄 Processing article 508/879...\n",
      "🔄 Processing article 509/879...\n",
      "🔄 Processing article 510/879...\n",
      "🔄 Processing article 511/879...\n",
      "🔄 Processing article 512/879...\n",
      "🔄 Processing article 513/879...\n",
      "🔄 Processing article 514/879...\n",
      "🔄 Processing article 515/879...\n",
      "🔄 Processing article 516/879...\n",
      "🔄 Processing article 517/879...\n",
      "🔄 Processing article 518/879...\n",
      "🔄 Processing article 519/879...\n",
      "🔄 Processing article 520/879...\n",
      "🔄 Processing article 521/879...\n",
      "🔄 Processing article 522/879...\n",
      "🔄 Processing article 523/879...\n",
      "🔄 Processing article 524/879...\n",
      "🔄 Processing article 525/879...\n",
      "🔄 Processing article 526/879...\n",
      "🔄 Processing article 527/879...\n",
      "🔄 Processing article 528/879...\n",
      "🔄 Processing article 529/879...\n",
      "🔄 Processing article 530/879...\n",
      "🔄 Processing article 531/879...\n",
      "🔄 Processing article 532/879...\n",
      "🔄 Processing article 533/879...\n",
      "🔄 Processing article 534/879...\n",
      "🔄 Processing article 535/879...\n",
      "🔄 Processing article 536/879...\n",
      "🔄 Processing article 537/879...\n",
      "🔄 Processing article 538/879...\n",
      "🔄 Processing article 539/879...\n",
      "🔄 Processing article 540/879...\n",
      "🔄 Processing article 541/879...\n",
      "🔄 Processing article 542/879...\n",
      "🔄 Processing article 543/879...\n",
      "🔄 Processing article 544/879...\n",
      "🔄 Processing article 545/879...\n",
      "🔄 Processing article 546/879...\n",
      "🔄 Processing article 547/879...\n",
      "🔄 Processing article 548/879...\n",
      "🔄 Processing article 549/879...\n",
      "🔄 Processing article 550/879...\n",
      "🔄 Processing article 551/879...\n",
      "🔄 Processing article 552/879...\n",
      "🔄 Processing article 553/879...\n",
      "🔄 Processing article 554/879...\n",
      "🔄 Processing article 555/879...\n",
      "🔄 Processing article 556/879...\n",
      "🔄 Processing article 557/879...\n",
      "🔄 Processing article 558/879...\n",
      "🔄 Processing article 559/879...\n",
      "🔄 Processing article 560/879...\n",
      "🔄 Processing article 561/879...\n",
      "🔄 Processing article 562/879...\n",
      "🔄 Processing article 563/879...\n",
      "🔄 Processing article 564/879...\n",
      "🔄 Processing article 565/879...\n",
      "🔄 Processing article 566/879...\n",
      "🔄 Processing article 567/879...\n",
      "🔄 Processing article 568/879...\n",
      "🔄 Processing article 569/879...\n",
      "🔄 Processing article 570/879...\n",
      "🔄 Processing article 571/879...\n",
      "🔄 Processing article 572/879...\n",
      "🔄 Processing article 573/879...\n",
      "🔄 Processing article 574/879...\n",
      "🔄 Processing article 575/879...\n",
      "🔄 Processing article 576/879...\n",
      "🔄 Processing article 577/879...\n",
      "🔄 Processing article 578/879...\n",
      "🔄 Processing article 579/879...\n",
      "🔄 Processing article 580/879...\n",
      "🔄 Processing article 581/879...\n",
      "🔄 Processing article 582/879...\n",
      "🔄 Processing article 583/879...\n",
      "🔄 Processing article 584/879...\n",
      "🔄 Processing article 585/879...\n",
      "🔄 Processing article 586/879...\n",
      "🔄 Processing article 587/879...\n",
      "🔄 Processing article 588/879...\n",
      "🔄 Processing article 589/879...\n",
      "🔄 Processing article 590/879...\n",
      "🔄 Processing article 591/879...\n",
      "🔄 Processing article 592/879...\n",
      "🔄 Processing article 593/879...\n",
      "🔄 Processing article 594/879...\n",
      "🔄 Processing article 595/879...\n",
      "🔄 Processing article 596/879...\n",
      "🔄 Processing article 597/879...\n",
      "🔄 Processing article 598/879...\n",
      "🔄 Processing article 599/879...\n",
      "🔄 Processing article 600/879...\n",
      "🔄 Processing article 601/879...\n",
      "🔄 Processing article 602/879...\n",
      "🔄 Processing article 603/879...\n",
      "🔄 Processing article 604/879...\n",
      "🔄 Processing article 605/879...\n",
      "🔄 Processing article 606/879...\n",
      "🔄 Processing article 607/879...\n",
      "🔄 Processing article 608/879...\n",
      "🔄 Processing article 609/879...\n",
      "🔄 Processing article 610/879...\n",
      "🔄 Processing article 611/879...\n",
      "🔄 Processing article 612/879...\n",
      "🔄 Processing article 613/879...\n",
      "🔄 Processing article 614/879...\n",
      "🔄 Processing article 615/879...\n",
      "🔄 Processing article 616/879...\n",
      "🔄 Processing article 617/879...\n",
      "🔄 Processing article 618/879...\n",
      "🔄 Processing article 619/879...\n",
      "🔄 Processing article 620/879...\n",
      "🔄 Processing article 621/879...\n",
      "🔄 Processing article 622/879...\n",
      "🔄 Processing article 623/879...\n",
      "🔄 Processing article 624/879...\n",
      "🔄 Processing article 625/879...\n",
      "🔄 Processing article 626/879...\n",
      "🔄 Processing article 627/879...\n",
      "🔄 Processing article 628/879...\n",
      "🔄 Processing article 629/879...\n",
      "🔄 Processing article 630/879...\n",
      "🔄 Processing article 631/879...\n",
      "🔄 Processing article 632/879...\n",
      "🔄 Processing article 633/879...\n",
      "🔄 Processing article 634/879...\n",
      "🔄 Processing article 635/879...\n",
      "🔄 Processing article 636/879...\n",
      "🔄 Processing article 637/879...\n",
      "🔄 Processing article 638/879...\n",
      "🔄 Processing article 639/879...\n",
      "🔄 Processing article 640/879...\n",
      "🔄 Processing article 641/879...\n",
      "🔄 Processing article 642/879...\n",
      "🔄 Processing article 643/879...\n",
      "🔄 Processing article 644/879...\n",
      "🔄 Processing article 645/879...\n",
      "🔄 Processing article 646/879...\n",
      "🔄 Processing article 647/879...\n",
      "🔄 Processing article 648/879...\n",
      "🔄 Processing article 649/879...\n",
      "🔄 Processing article 650/879...\n",
      "🔄 Processing article 651/879...\n",
      "🔄 Processing article 652/879...\n",
      "🔄 Processing article 653/879...\n",
      "🔄 Processing article 654/879...\n",
      "🔄 Processing article 655/879...\n",
      "🔄 Processing article 656/879...\n",
      "🔄 Processing article 657/879...\n",
      "🔄 Processing article 658/879...\n",
      "🔄 Processing article 659/879...\n",
      "🔄 Processing article 660/879...\n",
      "🔄 Processing article 661/879...\n",
      "🔄 Processing article 662/879...\n",
      "🔄 Processing article 663/879...\n",
      "🔄 Processing article 664/879...\n",
      "🔄 Processing article 665/879...\n",
      "🔄 Processing article 666/879...\n",
      "🔄 Processing article 667/879...\n",
      "🔄 Processing article 668/879...\n",
      "🔄 Processing article 669/879...\n",
      "🔄 Processing article 670/879...\n",
      "🔄 Processing article 671/879...\n",
      "🔄 Processing article 672/879...\n",
      "🔄 Processing article 673/879...\n",
      "🔄 Processing article 674/879...\n",
      "🔄 Processing article 675/879...\n",
      "🔄 Processing article 676/879...\n",
      "🔄 Processing article 677/879...\n",
      "🔄 Processing article 678/879...\n",
      "🔄 Processing article 679/879...\n",
      "🔄 Processing article 680/879...\n",
      "🔄 Processing article 681/879...\n",
      "🔄 Processing article 682/879...\n",
      "🔄 Processing article 683/879...\n",
      "🔄 Processing article 684/879...\n",
      "🔄 Processing article 685/879...\n",
      "🔄 Processing article 686/879...\n",
      "🔄 Processing article 687/879...\n",
      "🔄 Processing article 688/879...\n",
      "🔄 Processing article 689/879...\n",
      "🔄 Processing article 690/879...\n",
      "🔄 Processing article 691/879...\n",
      "🔄 Processing article 692/879...\n",
      "🔄 Processing article 693/879...\n",
      "🔄 Processing article 694/879...\n",
      "🔄 Processing article 695/879...\n",
      "🔄 Processing article 696/879...\n",
      "🔄 Processing article 697/879...\n",
      "🔄 Processing article 698/879...\n",
      "🔄 Processing article 699/879...\n",
      "🔄 Processing article 700/879...\n",
      "🔄 Processing article 701/879...\n",
      "🔄 Processing article 702/879...\n",
      "🔄 Processing article 703/879...\n",
      "🔄 Processing article 704/879...\n",
      "🔄 Processing article 705/879...\n",
      "🔄 Processing article 706/879...\n",
      "🔄 Processing article 707/879...\n",
      "🔄 Processing article 708/879...\n",
      "🔄 Processing article 709/879...\n",
      "🔄 Processing article 710/879...\n",
      "🔄 Processing article 711/879...\n",
      "🔄 Processing article 712/879...\n",
      "🔄 Processing article 713/879...\n",
      "🔄 Processing article 714/879...\n",
      "🔄 Processing article 715/879...\n",
      "🔄 Processing article 716/879...\n",
      "🔄 Processing article 717/879...\n",
      "🔄 Processing article 718/879...\n",
      "🔄 Processing article 719/879...\n",
      "🔄 Processing article 720/879...\n",
      "🔄 Processing article 721/879...\n",
      "🔄 Processing article 722/879...\n",
      "🔄 Processing article 723/879...\n",
      "🔄 Processing article 724/879...\n",
      "🔄 Processing article 725/879...\n",
      "🔄 Processing article 726/879...\n",
      "🔄 Processing article 727/879...\n",
      "🔄 Processing article 728/879...\n",
      "🔄 Processing article 729/879...\n",
      "🔄 Processing article 730/879...\n",
      "🔄 Processing article 731/879...\n",
      "🔄 Processing article 732/879...\n",
      "🔄 Processing article 733/879...\n",
      "🔄 Processing article 734/879...\n",
      "🔄 Processing article 735/879...\n",
      "🔄 Processing article 736/879...\n",
      "🔄 Processing article 737/879...\n",
      "🔄 Processing article 738/879...\n",
      "🔄 Processing article 739/879...\n",
      "🔄 Processing article 740/879...\n",
      "🔄 Processing article 741/879...\n",
      "🔄 Processing article 742/879...\n",
      "🔄 Processing article 743/879...\n",
      "🔄 Processing article 744/879...\n",
      "🔄 Processing article 745/879...\n",
      "🔄 Processing article 746/879...\n",
      "🔄 Processing article 747/879...\n",
      "🔄 Processing article 748/879...\n",
      "🔄 Processing article 749/879...\n",
      "🔄 Processing article 750/879...\n",
      "🔄 Processing article 751/879...\n",
      "🔄 Processing article 752/879...\n",
      "🔄 Processing article 753/879...\n",
      "🔄 Processing article 754/879...\n",
      "🔄 Processing article 755/879...\n",
      "🔄 Processing article 756/879...\n",
      "🔄 Processing article 757/879...\n",
      "🔄 Processing article 758/879...\n",
      "🔄 Processing article 759/879...\n",
      "🔄 Processing article 760/879...\n",
      "🔄 Processing article 761/879...\n",
      "🔄 Processing article 762/879...\n",
      "🔄 Processing article 763/879...\n",
      "🔄 Processing article 764/879...\n",
      "🔄 Processing article 765/879...\n",
      "🔄 Processing article 766/879...\n",
      "🔄 Processing article 767/879...\n",
      "🔄 Processing article 768/879...\n",
      "🔄 Processing article 769/879...\n",
      "🔄 Processing article 770/879...\n",
      "🔄 Processing article 771/879...\n",
      "🔄 Processing article 772/879...\n",
      "🔄 Processing article 773/879...\n",
      "🔄 Processing article 774/879...\n",
      "🔄 Processing article 775/879...\n",
      "🔄 Processing article 776/879...\n",
      "🔄 Processing article 777/879...\n",
      "🔄 Processing article 778/879...\n",
      "🔄 Processing article 779/879...\n",
      "🔄 Processing article 780/879...\n",
      "🔄 Processing article 781/879...\n",
      "🔄 Processing article 782/879...\n",
      "🔄 Processing article 783/879...\n",
      "🔄 Processing article 784/879...\n",
      "🔄 Processing article 785/879...\n",
      "🔄 Processing article 786/879...\n",
      "🔄 Processing article 787/879...\n",
      "🔄 Processing article 788/879...\n",
      "🔄 Processing article 789/879...\n",
      "🔄 Processing article 790/879...\n",
      "🔄 Processing article 791/879...\n",
      "🔄 Processing article 792/879...\n",
      "🔄 Processing article 793/879...\n",
      "🔄 Processing article 794/879...\n",
      "🔄 Processing article 795/879...\n",
      "🔄 Processing article 796/879...\n",
      "🔄 Processing article 797/879...\n",
      "🔄 Processing article 798/879...\n",
      "🔄 Processing article 799/879...\n",
      "🔄 Processing article 800/879...\n",
      "🔄 Processing article 801/879...\n",
      "🔄 Processing article 802/879...\n",
      "🔄 Processing article 803/879...\n",
      "🔄 Processing article 804/879...\n",
      "🔄 Processing article 805/879...\n",
      "🔄 Processing article 806/879...\n",
      "🔄 Processing article 807/879...\n",
      "🔄 Processing article 808/879...\n",
      "🔄 Processing article 809/879...\n",
      "🔄 Processing article 810/879...\n",
      "🔄 Processing article 811/879...\n",
      "🔄 Processing article 812/879...\n",
      "🔄 Processing article 813/879...\n",
      "🔄 Processing article 814/879...\n",
      "🔄 Processing article 815/879...\n",
      "🔄 Processing article 816/879...\n",
      "🔄 Processing article 817/879...\n",
      "🔄 Processing article 818/879...\n",
      "🔄 Processing article 819/879...\n",
      "🔄 Processing article 820/879...\n",
      "🔄 Processing article 821/879...\n",
      "🔄 Processing article 822/879...\n",
      "🔄 Processing article 823/879...\n",
      "🔄 Processing article 824/879...\n",
      "🔄 Processing article 825/879...\n",
      "🔄 Processing article 826/879...\n",
      "🔄 Processing article 827/879...\n",
      "🔄 Processing article 828/879...\n",
      "🔄 Processing article 829/879...\n",
      "🔄 Processing article 830/879...\n",
      "🔄 Processing article 831/879...\n",
      "🔄 Processing article 832/879...\n",
      "🔄 Processing article 833/879...\n",
      "🔄 Processing article 834/879...\n",
      "🔄 Processing article 835/879...\n",
      "🔄 Processing article 836/879...\n",
      "🔄 Processing article 837/879...\n",
      "🔄 Processing article 838/879...\n",
      "🔄 Processing article 839/879...\n",
      "🔄 Processing article 840/879...\n",
      "🔄 Processing article 841/879...\n",
      "🔄 Processing article 842/879...\n",
      "🔄 Processing article 843/879...\n",
      "🔄 Processing article 844/879...\n",
      "🔄 Processing article 845/879...\n",
      "🔄 Processing article 846/879...\n",
      "🔄 Processing article 847/879...\n",
      "🔄 Processing article 848/879...\n",
      "🔄 Processing article 849/879...\n",
      "🔄 Processing article 850/879...\n",
      "🔄 Processing article 851/879...\n",
      "🔄 Processing article 852/879...\n",
      "🔄 Processing article 853/879...\n",
      "🔄 Processing article 854/879...\n",
      "🔄 Processing article 855/879...\n",
      "🔄 Processing article 856/879...\n",
      "🔄 Processing article 857/879...\n",
      "🔄 Processing article 858/879...\n",
      "🔄 Processing article 859/879...\n",
      "🔄 Processing article 860/879...\n",
      "🔄 Processing article 861/879...\n",
      "🔄 Processing article 862/879...\n",
      "🔄 Processing article 863/879...\n",
      "🔄 Processing article 864/879...\n",
      "🔄 Processing article 865/879...\n",
      "🔄 Processing article 866/879...\n",
      "🔄 Processing article 867/879...\n",
      "🔄 Processing article 868/879...\n",
      "🔄 Processing article 869/879...\n",
      "🔄 Processing article 870/879...\n",
      "🔄 Processing article 871/879...\n",
      "🔄 Processing article 872/879...\n",
      "🔄 Processing article 873/879...\n",
      "🔄 Processing article 874/879...\n",
      "🔄 Processing article 875/879...\n",
      "🔄 Processing article 876/879...\n",
      "🔄 Processing article 877/879...\n",
      "🔄 Processing article 878/879...\n",
      "🔄 Processing article 879/879...\n",
      "\n",
      "📊 AVERAGE ROUGE F1 SCORES OVER TEST SAMPLES:\n",
      "ROUGE1: 0.1954\n",
      "ROUGE2: 0.0325\n",
      "ROUGEL: 0.1239\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Use only first 100 samples\n",
    "X_subset = X\n",
    "y_subset = y\n",
    "\n",
    "# Use last 10 from the subset\n",
    "X_test = X_subset\n",
    "y_test = y_subset\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "avg_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "# Generate summaries and calculate scores\n",
    "for i in range(len(X_test)):\n",
    "    print(f\"🔄 Processing article {i + 1}/{len(X_test)}...\")\n",
    "\n",
    "    summary = summarize_text(\n",
    "        text=X_test[i],\n",
    "        model=model,\n",
    "        source_tokenizer=e_tk,\n",
    "        word_dict=word_dict,\n",
    "        start_id=start_id,\n",
    "        end_id=end_id,\n",
    "        source_max=input_seq_len,\n",
    "        target_max=output_seq_len\n",
    "    )\n",
    "    \n",
    "    scores = scorer.score(y_test[i], summary)\n",
    "    for key in scores:\n",
    "        avg_scores[key].append(scores[key].fmeasure)\n",
    "\n",
    "# Compute and print average F1 scores\n",
    "print(\"\\n📊 AVERAGE ROUGE F1 SCORES OVER TEST SAMPLES:\")\n",
    "for key in avg_scores:\n",
    "    mean_f1 = sum(avg_scores[key]) / len(avg_scores[key])\n",
    "    print(f\"{key.upper()}: {mean_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "<h2 style='text-align:center;'>Observations</h2>\n",
    "\n",
    "+ As part of our course project, we implemented an attention-based Seq2Seq model for text summarization and evaluated it using ROUGE metrics.\n",
    "+ We initially experimented with a baseline model **without attention**, but it produced poor results both qualitatively and quantitatively. Due to its limitations, we decided not to pursue it further.\n",
    "+ In contrast, the attention-based model generated more coherent and contextually relevant summaries, despite occasional word repetitions or mid-sentence mix-ups.\n",
    "+ Although the model was more complex and took longer to train, the performance gains justified the added overhead.\n",
    "+ The ROUGE evaluation metrics clearly reflected the model’s effectiveness:\n",
    "\n",
    "  📊 **AVERAGE ROUGE F1 SCORES OVER FIRST 100 SAMPLES**:  \n",
    "  ‣ **ROUGE-1**: 0.1952  \n",
    "  ‣ **ROUGE-2**: 0.0322  \n",
    "  ‣ **ROUGE-L**: 0.1251  \n",
    "\n",
    "  📊 **AVERAGE ROUGE F1 SCORES OVER TEST SAMPLES** (after filtering):  \n",
    "  ‣ **ROUGE-1**: 0.1954  \n",
    "  ‣ **ROUGE-2**: 0.0325  \n",
    "  ‣ **ROUGE-L**: 0.1239  \n",
    "\n",
    "+ These results confirm the value of incorporating attention in sequence modeling and pave the way for exploring more advanced models like Transformers in future work.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1654566,
     "sourceId": 2734496,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7062438,
     "sourceId": 11294753,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7109217,
     "sourceId": 11359085,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7109591,
     "sourceId": 11359565,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
